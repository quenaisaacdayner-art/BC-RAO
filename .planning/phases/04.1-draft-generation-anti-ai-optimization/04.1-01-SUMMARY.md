---
phase: 04.1-draft-generation-anti-ai-optimization
plan: 01
subsystem: generation
tags: [prompt-engineering, openrouter, anti-ai-detection, humanization, structural-templates]

# Dependency graph
requires:
  - phase: 04-draft-generation
    provides: "Generation pipeline with PromptBuilder, InferenceClient, GenerationService"
provides:
  - "Positive HUMANIZATION_RULES replacing negation-based ANTI_AI_RULES"
  - "12 structural templates for random post shape selection per generation"
  - "8 randomized ending styles eliminating tidy-conclusion AI tell"
  - "Prose-based archetype guidance preventing bullet-point mirroring"
  - "Model parameters (frequency_penalty, presence_penalty) for creative variance"
  - "Forward-compatible opinion_landscape and imperfection_profile handling in prompt builder"
affects: [04.1-02-PLAN, 04.1-03-PLAN, generation-service, style-guide-generator]

# Tech tracking
tech-stack:
  added: []
  patterns:
    - "Positive prompt engineering with Good/Bad examples instead of NEVER-based negations"
    - "Random structural template injection for post shape diversity"
    - "Random ending style injection for natural post endings"
    - "Prose-format archetype guidance to prevent format mirroring"
    - "Forward-compatible .get() accessors for planned schema enrichments"

key-files:
  created: []
  modified:
    - "bc-rao-api/app/inference/router.py"
    - "bc-rao-api/app/inference/client.py"
    - "bc-rao-api/app/generation/prompt_builder.py"
    - "bc-rao-api/app/generation/generation_service.py"

key-decisions:
  - "Temperature 0.85 for all archetypes (up from 0.7) -- start uniform, add per-archetype tuning only if quality testing reveals measurable differences"
  - "frequency_penalty=0.3 and presence_penalty=0.2 for creative variance without incoherence"
  - "No top_p -- Claude Sonnet 4 rejects combined temperature+top_p (verified API constraint)"
  - "3-5 diverse Good/Bad examples per humanization rule to prevent LLM from copying any single example verbatim"
  - "Uniform random template selection (random.choice) sufficient for 12 templates -- no weighted selection needed"

patterns-established:
  - "Positive humanization: All writing rules use DO with examples instead of NEVER negations"
  - "Structural randomization: Every generation gets a random template name + shape injected"
  - "Ending randomization: Every generation gets a random ending instruction injected"
  - "Metadata tracking: structural_template and ending_style stored in generation_params JSONB"

# Metrics
duration: 9min
completed: 2026-02-13
---

# Phase 04.1 Plan 01: Prompt Input Pipeline Anti-AI Optimization Summary

**Positive humanization rules with 12 structural templates, 8 ending styles, and prose archetype guidance replacing negation-based ANTI_AI_RULES**

## Performance

- **Duration:** 9 min
- **Started:** 2026-02-13T02:35:06Z
- **Completed:** 2026-02-13T02:44:00Z
- **Tasks:** 2
- **Files modified:** 4

## Accomplishments
- Replaced all NEVER-based ANTI_AI_RULES with positive HUMANIZATION_RULES containing concrete Good/Bad examples across 4 sections (Structure, Language, Tone, Reddit-specific)
- Added 12 structural templates randomly selected per generation to prevent predictable post shapes
- Added 8 ending styles randomly injected to eliminate tidy-conclusion AI tell
- Rewrote all 3 archetype guidances (Journey, ProblemSolution, Feedback) as flowing prose paragraphs instead of bullet-point lists
- Added frequency_penalty=0.3 and presence_penalty=0.2 to generate_draft model config for creative variance
- Updated client.py to pass new parameters through to OpenRouter API with backward-compatible .get() defaults
- Added forward-compatible handling for opinion_landscape and imperfection_profile (Plan 03 enrichment)

## Task Commits

Each task was committed atomically:

1. **Task 1: Add model parameters to router.py and passthrough in client.py** - `4d45a5d` (feat)
2. **Task 2: Rewrite prompt_builder.py with positive rules, structural templates, ending styles, and prose archetype guidance** - `42c7736` (feat)
3. **Deviation: Capture structural template metadata in generation_params** - `a9f236d` (fix)

## Files Created/Modified
- `bc-rao-api/app/inference/router.py` - Updated generate_draft config with temperature=0.85, frequency_penalty=0.3, presence_penalty=0.2
- `bc-rao-api/app/inference/client.py` - Added frequency_penalty and presence_penalty passthrough to OpenRouter API payload
- `bc-rao-api/app/generation/prompt_builder.py` - Complete rewrite: HUMANIZATION_RULES, 12 STRUCTURAL_TEMPLATES, 8 ENDING_STYLES, prose archetype guidance, forward-compatible style guide/metrics handling
- `bc-rao-api/app/generation/generation_service.py` - Capture structural_template and ending_style metadata in generation_params JSONB

## Decisions Made
- Temperature 0.85 for all archetypes: start uniform, add per-archetype tuning only if quality testing reveals measurable differences
- frequency_penalty=0.3 and presence_penalty=0.2: balances creative variance without incoherence
- No top_p: Claude Sonnet 4 rejects combined temperature+top_p (verified API constraint, returns 400 error)
- 3-5 diverse examples per humanization rule: prevents LLM from copying any single example verbatim
- Uniform random template selection: with 12 templates, random.choice is sufficient (no weighted selection needed)
- Changed style_guide vocabulary "NEVER use" to "Avoid" for consistency with positive-instruction approach

## Deviations from Plan

### Auto-fixed Issues

**1. [Rule 2 - Missing Critical] Capture structural template metadata in generation_service.py**
- **Found during:** Task 2 (after prompt_builder rewrite)
- **Issue:** prompt_builder.build_prompt() now returns metadata dict with structural_template and ending_style, but generation_service.py was not capturing this metadata in generation_params JSONB, making it impossible to debug which templates/endings were used per draft
- **Fix:** Added metadata extraction from prompts dict into generation_params with safe .get() accessor for backward compatibility
- **Files modified:** bc-rao-api/app/generation/generation_service.py
- **Verification:** Import verified, metadata dict correctly merged into generation_params
- **Committed in:** a9f236d

---

**Total deviations:** 1 auto-fixed (1 missing critical)
**Impact on plan:** Essential for debugging/monitoring which templates and endings are used. No scope creep.

## Issues Encountered
- Verification commands requiring Supabase connection (InferenceClient constructor calls CostTracker which requires SUPABASE_URL) -- verified using inspect.signature instead of instantiation. Not a code issue, just a test environment constraint.
- Pre-existing test failures in test_collection_service.py, test_cost_tracker.py, and test_regex_filter.py are unrelated to our changes. All 97 generation-related tests pass (test_blacklist_validator, test_isc_gating, test_style_extractor, test_style_guide_generator).

## User Setup Required
None - no external service configuration required.

## Next Phase Readiness
- Prompt input pipeline fully optimized for anti-AI generation
- Plan 02 (blacklist_validator.py + generation_service.py regeneration loop) can proceed immediately -- it builds on the improved prompts from this plan
- Plan 03 (community DNA enrichment) can proceed -- forward-compatible .get() handlers for opinion_landscape and imperfection_profile already in place
- structural_template and ending_style metadata now tracked in generation_params for future A/B analysis

---
*Phase: 04.1-draft-generation-anti-ai-optimization*
*Completed: 2026-02-13*
