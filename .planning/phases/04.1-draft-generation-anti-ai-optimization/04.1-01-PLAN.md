---
phase: 04.1-draft-generation-anti-ai-optimization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - bc-rao-api/app/inference/router.py
  - bc-rao-api/app/inference/client.py
  - bc-rao-api/app/generation/prompt_builder.py
autonomous: true

must_haves:
  truths:
    - "Generation prompts use positive instructions with concrete before/after examples instead of NEVER-based negations"
    - "Each generation randomly selects from 10+ structural templates preventing predictable post shapes"
    - "Posts end with randomized ending styles (abrupt stop, specific question, frustrated aside, trailing off) instead of tidy conclusions"
    - "Model parameters include frequency_penalty and presence_penalty for creative variance"
    - "Archetype guidance is written as flowing prose, not bullet-point lists"
  artifacts:
    - path: "bc-rao-api/app/inference/router.py"
      provides: "Optimized model parameters for generate_draft task"
      contains: "frequency_penalty"
    - path: "bc-rao-api/app/inference/client.py"
      provides: "Passthrough of frequency_penalty and presence_penalty to OpenRouter API"
      contains: "frequency_penalty"
    - path: "bc-rao-api/app/generation/prompt_builder.py"
      provides: "Positive humanization rules, 12 structural templates, 8 ending styles, prose archetype guidance"
      contains: "STRUCTURAL_TEMPLATES"
  key_links:
    - from: "bc-rao-api/app/inference/router.py"
      to: "bc-rao-api/app/inference/client.py"
      via: "MODEL_ROUTING config dict consumed by InferenceClient"
      pattern: "self\\.config\\[.frequency_penalty.\\]"
    - from: "bc-rao-api/app/inference/client.py"
      to: "OpenRouter API"
      via: "payload dict in _call_openrouter"
      pattern: "frequency_penalty.*payload"
    - from: "bc-rao-api/app/generation/prompt_builder.py"
      to: "bc-rao-api/app/generation/generation_service.py"
      via: "build_prompt() returns system+user messages"
      pattern: "STRUCTURAL_TEMPLATES|ENDING_STYLES"
---

<objective>
Overhaul the generation input pipeline: upgrade model parameters for creative variance, replace negation-based ANTI_AI_RULES with positive humanization instructions containing concrete before/after examples, add 12 structural templates for random post shape selection, add 8 randomized ending styles, and rewrite archetype guidance as flowing prose instead of bullet-point lists.

Purpose: This is the highest-impact change. Research confirms LLMs perform 3-5x worse at following "NEVER do X" than "DO Y instead." Structural templates prevent predictable post shapes. Ending randomization eliminates the tidy-conclusion AI tell. Prose-based archetype guidance prevents the LLM from mirroring bullet-point format in output.

Output: Updated router.py, client.py, and prompt_builder.py with anti-AI optimizations.
</objective>

<execution_context>
@C:/Users/quena/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/quena/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.1-draft-generation-anti-ai-optimization/04.1-RESEARCH.md
@.planning/debug/generic-ai-detectable-drafts.md
@bc-rao-api/app/inference/router.py
@bc-rao-api/app/inference/client.py
@bc-rao-api/app/generation/prompt_builder.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add model parameters to router.py and passthrough in client.py</name>
  <files>bc-rao-api/app/inference/router.py, bc-rao-api/app/inference/client.py</files>
  <action>
**router.py:** Update the `generate_draft` entry in MODEL_ROUTING to include optimized parameters:
- Change temperature from 0.7 to 0.85 (more creative variance)
- Add `"frequency_penalty": 0.3` (discourage repetitive phrasing)
- Add `"presence_penalty": 0.2` (encourage topic diversity)
- Do NOT add top_p — Claude Sonnet 4 rejects combined temperature+top_p (verified API constraint, returns 400 error)
- Leave all other task configs (classify_archetype, score_post, extract_patterns) unchanged

**client.py:** Update `_call_openrouter()` method to accept and pass optional parameters from the config dict:
1. Add `frequency_penalty: Optional[float] = None` and `presence_penalty: Optional[float] = None` parameters to `_call_openrouter()` method signature
2. In the payload dict construction, add conditional inclusion:
   ```python
   if frequency_penalty is not None:
       payload["frequency_penalty"] = frequency_penalty
   if presence_penalty is not None:
       payload["presence_penalty"] = presence_penalty
   ```
3. Update the `call()` method to extract these params from `self.config` and pass them to `_call_openrouter()`:
   ```python
   result = await self._call_openrouter(
       messages=messages,
       model=self.config["model"],
       max_tokens=self.config["max_tokens"],
       temperature=self.config["temperature"],
       frequency_penalty=self.config.get("frequency_penalty"),
       presence_penalty=self.config.get("presence_penalty"),
   )
   ```
   Apply same pattern to both primary and fallback calls in the try/except block.

Do NOT break existing task configs that lack these parameters — the `.get()` with None default ensures backward compatibility.
  </action>
  <verify>
Run `python -c "from app.inference.router import MODEL_ROUTING; cfg = MODEL_ROUTING['generate_draft']; assert cfg['temperature'] == 0.85; assert cfg['frequency_penalty'] == 0.3; assert cfg['presence_penalty'] == 0.2; assert 'top_p' not in cfg; print('router OK')"` from bc-rao-api directory.

Run `python -c "from app.inference.client import InferenceClient; c = InferenceClient(task='generate_draft'); print('client OK')"` from bc-rao-api directory.

Run `python -c "from app.inference.client import InferenceClient; c = InferenceClient(task='classify_archetype'); print('backward compat OK')"` to verify non-generation tasks still work.
  </verify>
  <done>
generate_draft config has temperature=0.85, frequency_penalty=0.3, presence_penalty=0.2, no top_p. client.py passes these to OpenRouter payload. Other task configs unaffected.
  </done>
</task>

<task type="auto">
  <name>Task 2: Rewrite prompt_builder.py with positive rules, structural templates, ending styles, and prose archetype guidance</name>
  <files>bc-rao-api/app/generation/prompt_builder.py</files>
  <action>
This is a comprehensive rewrite of the prompt construction logic. Read the existing prompt_builder.py first to understand all method signatures and callers.

**A. Replace ANTI_AI_RULES with HUMANIZATION_RULES:**

Replace the entire ANTI_AI_RULES class constant with a new HUMANIZATION_RULES constant that uses positive instructions with concrete before/after examples. The new rules must cover the same 4 sections (STRUCTURE, LANGUAGE, TONE, REDDIT-SPECIFIC) but rewritten as "DO this" with Good/Bad examples instead of "NEVER do that."

Use 3-5 diverse examples per rule to prevent the LLM from copying any single example verbatim. Add the explicit note: "These examples show the ENERGY and VIBE, not exact words to copy."

Key transformations:
- "NEVER use bullet points" → "Write in flowing paragraphs like a person typing quickly" with Good/Bad examples
- "NEVER use a hook, body, conclusion structure" → "Start mid-thought. Let your post trail off or end abruptly" with Good/Bad examples
- "NEVER use formal transitions" → "Connect thoughts like you're talking, not writing an essay" with examples
- "NEVER use corporate language" → "Sound like a tired developer, not a press release" with examples
- Preserve all the actual banned phrases as Bad examples rather than NEVER lists

Update the reference in build_prompt() and _build_generic_prompt() from `self.ANTI_AI_RULES` to `self.HUMANIZATION_RULES`.

**B. Add STRUCTURAL_TEMPLATES list:**

Add a module-level or class-level list of 12 structural templates. Each template is a dict with keys: `name` (str), `instruction` (str), `example_shape` (str). Use the 12 templates defined in the research (04.1-RESEARCH.md Pattern 2): climax_first, tangent_heavy, mid_rant, stream_of_consciousness, buried_lede, list_disguised, question_driven, frustration_dump, update_style, discovery_story, comparison_rant, reluctant_recommendation.

Add `import random` at the top of the file.

**C. Add ENDING_STYLES list:**

Add a list of 8 ending style instructions as strings. Use the 8 styles from research: abrupt stop, specific question, frustrated aside, trailing off, tangent pivot, self-deprecation, edit/update addendum, mid-thought submit.

**D. Rewrite _get_archetype_guidance() as flowing prose:**

Rewrite all 3 archetype guidance strings (Journey, ProblemSolution, Feedback) as flowing prose paragraphs instead of bullet-point lists. This is critical because LLMs mirror instruction formatting in output — bullet-point guidance produces bullet-point posts.

For Journey: Write 2-3 paragraphs describing the personal discovery story energy. Describe the emotional flow without using enumerated structure. Emphasize starting mid-chaos, specificity over generality, the solution emerging organically.

For ProblemSolution: Write 2-3 paragraphs about raw pain-first writing. The problem dominates, the solution is an afterthought. No balanced presentation.

For Feedback: Write 2-3 paragraphs about inverting authority — positioning as learner, asking specific questions, showing real uncertainty.

Remove the bullet-point format entirely from all three archetypes.

**E. Update build_prompt() to inject structural template + ending style:**

In build_prompt() method:
1. At the start, select a random structural template: `template = random.choice(self.STRUCTURAL_TEMPLATES)`
2. Select a random ending style: `ending = random.choice(self.ENDING_STYLES)`
3. Add a new section in the system message after the archetype guidance:
   ```
   ## Post Structure (MANDATORY)
   {template["instruction"]}
   Shape: {template["example_shape"]}
   ```
4. Add ending instruction to the user message:
   ```
   ENDING INSTRUCTION: {ending}
   ```
5. Store the selected template name in the return dict for debugging: add a third key `"metadata"` with `{"structural_template": template["name"], "ending_style": ending[:30]}`

Apply the same structural template and ending injection to `_build_generic_prompt()`.

**F. Update _format_style_guide() to handle new fields gracefully:**

Add `.get()` accessors for the new fields that Plan 03 will add (opinion_landscape, imperfection_profile) so that:
- If the style guide has `opinion_landscape`, format and inject it
- If the style guide has `imperfection_profile`, format and inject it
- If these fields are absent (old profiles), skip silently — no KeyError

This is forward-compatible preparation so Plan 03 enrichment works immediately.

Add after the existing taboo_patterns section:
```python
opinion = style_guide.get("opinion_landscape", {})
if opinion:
    sections.append("\nCOMMUNITY OPINIONS:")
    if opinion.get("loved_tools"):
        sections.append(f"  Champions: {', '.join(opinion['loved_tools'])}")
    if opinion.get("hated_tools"):
        sections.append(f"  Despises: {', '.join(opinion['hated_tools'])}")
    if opinion.get("controversial_takes"):
        sections.append(f"  Debates: {', '.join(opinion['controversial_takes'])}")
    if opinion.get("tribal_knowledge"):
        sections.append(f"  Insider refs: {', '.join(opinion['tribal_knowledge'])}")

imperfection = style_guide.get("imperfection_profile", {})
if imperfection:
    sections.append("\nIMPERFECTION PROFILE:")
    for key in ["typical_typos", "grammar_looseness", "self_correction_frequency", "digression_tolerance"]:
        if imperfection.get(key):
            sections.append(f"  {key.replace('_', ' ').title()}: {imperfection[key]}")
```

**G. Update _format_style_metrics() for imperfection metrics:**

Add a new section at the end of _format_style_metrics() that formats imperfection data if present:
```python
imperfections = style_metrics.get("imperfections", {})
if imperfections:
    parts = []
    fr = imperfections.get("fragment_ratio", 0)
    if fr > 0.05:
        parts.append(f"sentence fragments ({fr*100:.0f}% of sentences)")
    pr = imperfections.get("parenthetical_frequency", 0)
    if pr > 0.1:
        parts.append(f"parenthetical asides ({pr:.1f}/post)")
    sc = imperfections.get("self_correction_rate", 0)
    if sc > 0.05:
        parts.append(f"self-corrections ({sc:.1f}/post)")
    dr = imperfections.get("dash_interruption_rate", 0)
    if dr > 0.1:
        parts.append(f"dash interruptions ({dr:.1f}/post)")
    if parts:
        sections.append(f"\nImperfection patterns: {', '.join(parts)}")
```

This is forward-compatible for Plan 03's style_extractor.py changes.
  </action>
  <verify>
Run `python -c "from app.generation.prompt_builder import PromptBuilder; pb = PromptBuilder(); p = pb.build_prompt('python', 'Journey', 'Building a web scraper'); assert 'HUMANIZATION_RULES' not in dir(pb) or hasattr(pb, 'HUMANIZATION_RULES'); assert 'ANTI_AI_RULES' not in p['system']; assert 'metadata' in p; assert 'structural_template' in p['metadata']; print('prompt_builder OK')"` from bc-rao-api directory.

Verify STRUCTURAL_TEMPLATES has 12 entries: `python -c "from app.generation.prompt_builder import PromptBuilder; assert len(PromptBuilder.STRUCTURAL_TEMPLATES) >= 12; print(f'{len(PromptBuilder.STRUCTURAL_TEMPLATES)} templates OK')"`.

Verify ENDING_STYLES has 8 entries: `python -c "from app.generation.prompt_builder import PromptBuilder; assert len(PromptBuilder.ENDING_STYLES) >= 8; print(f'{len(PromptBuilder.ENDING_STYLES)} endings OK')"`.

Verify no bullet-point archetype guidance: `python -c "from app.generation.prompt_builder import PromptBuilder; pb = PromptBuilder(); g = pb._get_archetype_guidance('Journey', 5.0); assert '- ' not in g[:100]; print('prose guidance OK')"`.

Verify generic prompt also gets templates: `python -c "from app.generation.prompt_builder import PromptBuilder; pb = PromptBuilder(); p = pb.build_prompt('test', 'Feedback', 'test context'); assert 'Post Structure' in p['system']; print('generic template OK')"`.
  </verify>
  <done>
ANTI_AI_RULES replaced with positive HUMANIZATION_RULES containing Good/Bad examples. 12 structural templates randomly selected per generation. 8 ending styles randomly injected. Archetype guidance rewritten as flowing prose (no bullet points). build_prompt() returns metadata with template name. Style guide and metrics formatters handle new community DNA fields gracefully.
  </done>
</task>

</tasks>

<verification>
1. Import all modified modules without errors: `python -c "from app.inference.router import MODEL_ROUTING; from app.inference.client import InferenceClient; from app.generation.prompt_builder import PromptBuilder; print('All imports OK')"`
2. Generate a prompt and verify it contains structural template, ending instruction, and positive rules (no NEVER-based negations in the humanization section)
3. Verify model parameters: temperature=0.85, frequency_penalty=0.3, presence_penalty=0.2, no top_p
4. Run existing tests: `cd bc-rao-api && python -m pytest tests/ -x -q`
</verification>

<success_criteria>
- ANTI_AI_RULES constant no longer exists or is replaced by HUMANIZATION_RULES with positive instructions
- Every build_prompt() call includes a randomly selected structural template and ending style
- Archetype guidance uses flowing prose paragraphs, not bullet-point lists
- router.py generate_draft config includes frequency_penalty and presence_penalty
- client.py passes new parameters to OpenRouter API payload
- All existing imports and function signatures remain backward-compatible
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-draft-generation-anti-ai-optimization/04.1-01-SUMMARY.md`
</output>
