---
phase: 04.1-draft-generation-anti-ai-optimization
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - bc-rao-api/app/analysis/style_extractor.py
  - bc-rao-api/app/analysis/style_guide_generator.py
  - bc-rao-api/app/analysis/scorers.py
  - bc-rao-api/tests/test_style_extractor.py
  - bc-rao-api/tests/test_style_guide_generator.py
autonomous: true

must_haves:
  truths:
    - "Style extractor captures imperfection metrics (fragment ratio, parenthetical frequency, self-correction rate, dash interruption rate)"
    - "Style guide generator requests opinion landscape data (loved/hated tools, controversial takes, tribal knowledge) from LLM"
    - "Burstiness score measures sentence length variance and compares to community baseline"
    - "All new fields use .get() with defaults — old profiles without these fields work without errors"
  artifacts:
    - path: "bc-rao-api/app/analysis/style_extractor.py"
      provides: "Imperfection metrics extraction via SpaCy POS tags and regex"
      contains: "_extract_imperfections"
    - path: "bc-rao-api/app/analysis/style_guide_generator.py"
      provides: "Opinion landscape and imperfection profile fields in LLM style guide"
      contains: "opinion_landscape"
    - path: "bc-rao-api/app/analysis/scorers.py"
      provides: "Burstiness score calculation using coefficient of variation"
      contains: "calculate_burstiness_score"
  key_links:
    - from: "bc-rao-api/app/analysis/style_extractor.py"
      to: "bc-rao-api/app/generation/prompt_builder.py"
      via: "imperfections dict stored in community_profiles.style_metrics, read by _format_style_metrics()"
      pattern: "imperfections.*fragment_ratio"
    - from: "bc-rao-api/app/analysis/style_guide_generator.py"
      to: "bc-rao-api/app/generation/prompt_builder.py"
      via: "opinion_landscape dict stored in community_profiles.style_guide, read by _format_style_guide()"
      pattern: "opinion_landscape.*loved_tools"
    - from: "bc-rao-api/app/analysis/scorers.py"
      to: "bc-rao-api/app/generation/generation_service.py"
      via: "calculate_burstiness_score() can be called in Step 7 scoring"
      pattern: "calculate_burstiness_score"
---

<objective>
Enrich community intelligence with imperfection metrics (SpaCy-extracted fragment ratio, parenthetical frequency, self-correction patterns, dash interruptions), opinion landscape data (LLM-generated loved/hated tools, controversial takes, tribal knowledge), and burstiness scoring (sentence length variance measured as coefficient of variation).

Purpose: The generation pipeline needs richer community data to produce truly human-like posts. Imperfection metrics tell the LLM how messy to write. Opinion landscape gives the LLM community biases to adopt. Burstiness scoring measures whether generated text has human-like sentence length variance (humans: CV ~0.5-0.8, AI: CV ~0.2-0.4).

Output: Updated style_extractor.py, style_guide_generator.py, scorers.py, and their tests.
</objective>

<execution_context>
@C:/Users/quena/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/quena/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.1-draft-generation-anti-ai-optimization/04.1-RESEARCH.md
@.planning/debug/generic-ai-detectable-drafts.md
@bc-rao-api/app/analysis/style_extractor.py
@bc-rao-api/app/analysis/style_guide_generator.py
@bc-rao-api/app/analysis/scorers.py
@bc-rao-api/tests/test_style_extractor.py
@bc-rao-api/tests/test_style_guide_generator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add imperfection metrics to style_extractor.py and opinion landscape to style_guide_generator.py</name>
  <files>bc-rao-api/app/analysis/style_extractor.py, bc-rao-api/app/analysis/style_guide_generator.py, bc-rao-api/tests/test_style_extractor.py, bc-rao-api/tests/test_style_guide_generator.py</files>
  <action>
Read existing tests FIRST (test_style_extractor.py and test_style_guide_generator.py) to understand what they assert.

**style_extractor.py — Add _extract_imperfections() helper and integrate into extract_community_style():**

1. Add a new function `_extract_imperfections(docs, texts) -> Dict[str, Any]` that calculates 4 metrics:

   - **fragment_ratio**: Ratio of sentences that have no VERB POS tag (sentence fragments). Iterate through `doc.sents` for each doc, check if any token in the sentence has `token.pos_ == "VERB"`. Sentences without a verb are fragments. Handle edge cases: skip single-word sentences and sentences that are just punctuation.

   - **parenthetical_frequency**: Average count of parenthetical asides per post. Use regex `r'\([^)]{5,}\)'` to find parenthetical text that is 5+ chars (filters out emoticons and short asides). Count per text, average over all texts.

   - **self_correction_rate**: Average count of self-correction markers per post. Use regex `r'\b(?:I mean|actually|wait|edit:|update:|sorry,? I meant)\b'` (case-insensitive). Count per text, average over all texts.

   - **dash_interruption_rate**: Average count of mid-sentence dash interruptions per post. Use regex `r'\s[-\u2013\u2014]{1,2}\s'` (space-dash-space or em-dash). Count per text, average over all texts.

   Return dict:
   ```python
   {
       "fragment_ratio": round(fragment_count / total_sentences, 3) if total_sentences > 0 else 0.0,
       "parenthetical_frequency": round(sum(parenthetical_counts) / n, 2) if n > 0 else 0.0,
       "self_correction_rate": round(sum(self_correction_counts) / n, 2) if n > 0 else 0.0,
       "dash_interruption_rate": round(sum(dash_counts) / n, 2) if n > 0 else 0.0,
   }
   ```

2. Call `_extract_imperfections(docs, texts)` in `extract_community_style()` and add to the return dict:
   ```python
   imperfections = _extract_imperfections(docs, texts)
   # ... in return dict:
   return {
       "vocabulary": vocabulary,
       "structure": structure,
       "punctuation": punctuation,
       "formatting": formatting,
       "openings": openings,
       "imperfections": imperfections,  # NEW
   }
   ```

3. Update `_empty_style()` to include `"imperfections"` key with zero-value defaults:
   ```python
   "imperfections": {
       "fragment_ratio": 0.0,
       "parenthetical_frequency": 0.0,
       "self_correction_rate": 0.0,
       "dash_interruption_rate": 0.0,
   },
   ```

**style_guide_generator.py — Add opinion_landscape and imperfection_profile to LLM prompt:**

1. Update `USER_PROMPT_TEMPLATE` to add new fields to the Required Output JSON schema. Add these two new fields to the JSON structure in the template:
   ```
   "opinion_landscape": {{
       "loved_tools": ["tools/frameworks this community champions — be specific"],
       "hated_tools": ["tools/frameworks this community despises or mocks"],
       "controversial_takes": ["divisive opinions that spark debate in this community"],
       "tribal_knowledge": ["insider references, memes, running jokes unique to this community"],
       "strong_biases": ["clear community biases, e.g. 'prefers X over Y'"]
   }},
   "imperfection_profile": {{
       "typical_typos": "whether typos are common or rare in this community",
       "grammar_looseness": "how loose grammar is (fragments, run-ons, etc)",
       "self_correction_frequency": "how often people correct themselves mid-post",
       "digression_tolerance": "how much the community tolerates off-topic tangents"
   }}
   ```

2. Update `EMPTY_STYLE_GUIDE` to include these new fields with empty defaults:
   ```python
   "opinion_landscape": {"loved_tools": [], "hated_tools": [], "controversial_takes": [], "tribal_knowledge": [], "strong_biases": []},
   "imperfection_profile": {"typical_typos": "", "grammar_looseness": "", "self_correction_frequency": "", "digression_tolerance": ""},
   ```

3. Update `_validate_style_guide()` to handle the new fields:
   - Add `"imperfection_profile"` to the dict-with-strings validation (similar to vocabulary_guide but with string values)
   - Add `"opinion_landscape"` to the dict-with-lists validation (similar to vocabulary_guide)

   For imperfection_profile:
   ```python
   if "imperfection_profile" in parsed and isinstance(parsed["imperfection_profile"], dict):
       ip = parsed["imperfection_profile"]
       result["imperfection_profile"] = {
           "typical_typos": ip.get("typical_typos", "") if isinstance(ip.get("typical_typos"), str) else "",
           "grammar_looseness": ip.get("grammar_looseness", "") if isinstance(ip.get("grammar_looseness"), str) else "",
           "self_correction_frequency": ip.get("self_correction_frequency", "") if isinstance(ip.get("self_correction_frequency"), str) else "",
           "digression_tolerance": ip.get("digression_tolerance", "") if isinstance(ip.get("digression_tolerance"), str) else "",
       }
   ```

   For opinion_landscape:
   ```python
   if "opinion_landscape" in parsed and isinstance(parsed["opinion_landscape"], dict):
       ol = parsed["opinion_landscape"]
       result["opinion_landscape"] = {
           "loved_tools": ol.get("loved_tools", []) if isinstance(ol.get("loved_tools"), list) else [],
           "hated_tools": ol.get("hated_tools", []) if isinstance(ol.get("hated_tools"), list) else [],
           "controversial_takes": ol.get("controversial_takes", []) if isinstance(ol.get("controversial_takes"), list) else [],
           "tribal_knowledge": ol.get("tribal_knowledge", []) if isinstance(ol.get("tribal_knowledge"), list) else [],
           "strong_biases": ol.get("strong_biases", []) if isinstance(ol.get("strong_biases"), list) else [],
       }
   ```

**Update tests:**

- test_style_extractor.py: Add test that `extract_community_style()` returns an `imperfections` key with all 4 metrics. Test with text containing fragments ("Beautiful day."), parentheticals ("something (which was surprising) happened"), self-corrections ("I mean, actually it was fine"), and dashes ("the thing -- you know -- worked").

- test_style_guide_generator.py: Update the expected EMPTY_STYLE_GUIDE structure to include the new fields. If tests assert specific keys, add the new keys.
  </action>
  <verify>
Run `cd bc-rao-api && python -m pytest tests/test_style_extractor.py -x -v` — all tests must pass.

Run `cd bc-rao-api && python -m pytest tests/test_style_guide_generator.py -x -v` — all tests must pass.

Run `python -c "from app.analysis.style_extractor import extract_community_style; r = extract_community_style(['This is great. Beautiful sunset.', 'I mean, actually -- this thing (which was odd) broke.']); assert 'imperfections' in r; assert 'fragment_ratio' in r['imperfections']; print(f'imperfections: {r[\"imperfections\"]}')"` from bc-rao-api directory.

Run `python -c "from app.analysis.style_guide_generator import EMPTY_STYLE_GUIDE; assert 'opinion_landscape' in EMPTY_STYLE_GUIDE; assert 'imperfection_profile' in EMPTY_STYLE_GUIDE; print('style guide schema OK')"` from bc-rao-api directory.
  </verify>
  <done>
style_extractor.py extracts 4 imperfection metrics via SpaCy POS tags and regex. style_guide_generator.py requests opinion_landscape (5 fields) and imperfection_profile (4 fields) from LLM. Both use .get() with defaults for backward compatibility with existing profiles. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add burstiness score to scorers.py</name>
  <files>bc-rao-api/app/analysis/scorers.py</files>
  <action>
**Add calculate_burstiness_score() function** to scorers.py:

```python
def calculate_burstiness_score(
    post_sentence_length_std: Optional[float],
    post_avg_sentence_length: Optional[float],
    community_burstiness_cv: Optional[float] = None,
) -> float:
    """
    Score how bursty (sentence-length-varied) a post is.

    Burstiness is measured via Coefficient of Variation (CV = std_dev / mean).
    Human text typically has CV 0.5-0.8, AI text has CV 0.2-0.4.

    Args:
        post_sentence_length_std: Standard deviation of sentence lengths in post
        post_avg_sentence_length: Mean sentence length in post
        community_burstiness_cv: Community's average CV (if available)

    Returns:
        Score from 0-10 where 10 = matches community burstiness or falls in human-like range.
    """
    if post_sentence_length_std is None or post_avg_sentence_length is None:
        return 5.0  # Neutral if data missing
    if post_avg_sentence_length == 0:
        return 0.0

    post_cv = post_sentence_length_std / post_avg_sentence_length

    if community_burstiness_cv is not None:
        # Score based on distance from community CV
        diff = abs(post_cv - community_burstiness_cv)
        return max(0.0, min(10.0, round(10.0 - diff * 15.0, 2)))

    # No community reference: score based on absolute CV
    # CV < 0.3 = too uniform (AI-like), CV > 0.9 = too chaotic
    if post_cv < 0.3:
        return round(post_cv * 20.0, 2)  # 0-6 range for low CV
    elif post_cv > 0.9:
        return round(max(0.0, 10.0 - (post_cv - 0.9) * 20.0), 2)
    else:
        return round(min(10.0, 6.0 + (post_cv - 0.3) * 6.67), 2)  # 6-10 range for good CV
```

**Add `from typing import Optional`** if not already imported (it is already imported).

**Update calculate_post_score()** to include burstiness as a new factor:

1. Calculate burstiness in the function:
   ```python
   # Calculate burstiness
   burstiness = calculate_burstiness_score(
       post_data.get("sentence_length_std"),
       post_data.get("avg_sentence_length"),
       community_avg.get("burstiness_cv"),  # Computed on-the-fly from profile if available
   )
   ```

2. Update the total score formula to include burstiness. Adjust weights to maintain 0-10 range:
   ```python
   # Updated formula with burstiness
   total = (
       vulnerability * 0.20
       + rhythm * 0.20
       + formality * 0.15
       + burstiness * 0.15
       - jargon_penalty * 0.15
       - link_penalty * 0.15
   )
   ```
   Note: vulnerability weight reduced from 0.25 to 0.20, rhythm from 0.25 to 0.20, formality from 0.20 to 0.15. This makes room for burstiness at 0.15 while keeping penalties unchanged. Total positive weights = 0.70, total penalty weights = 0.30, same as before.

3. Add burstiness to the return dict:
   ```python
   return {
       "vulnerability_weight": vulnerability,
       "rhythm_adherence": rhythm,
       "formality_match": formality,
       "burstiness_score": burstiness,  # NEW
       "marketing_jargon_penalty": jargon_penalty,
       "link_density_penalty": link_penalty,
       "total_score": round(total, 2),
       "penalty_phrases": penalty_phrases,
   }
   ```

**Important backward compatibility note:** The updated calculate_post_score() adds a new key `burstiness_score` to the return dict. All callers use dict access patterns, so adding a key is safe. The weight adjustments change the total_score calculation, but this is intentional — burstiness is a quality improvement. The community_avg dict gains a new optional key `burstiness_cv` which is computed on-the-fly from `avg_sentence_length` and `sentence_length_std` — callers that don't provide it get the default scoring.
  </action>
  <verify>
Run `python -c "from app.analysis.scorers import calculate_burstiness_score; s = calculate_burstiness_score(8.0, 15.0); assert 0 <= s <= 10; print(f'burstiness score: {s}')"` from bc-rao-api directory.

Test human-like CV scores high: `python -c "from app.analysis.scorers import calculate_burstiness_score; s = calculate_burstiness_score(9.0, 15.0); print(f'human-like CV 0.6: score={s}'); assert s >= 6.0"` from bc-rao-api directory.

Test AI-like CV scores low: `python -c "from app.analysis.scorers import calculate_burstiness_score; s = calculate_burstiness_score(3.0, 15.0); print(f'AI-like CV 0.2: score={s}'); assert s < 6.0"` from bc-rao-api directory.

Verify calculate_post_score includes burstiness: `python -c "from app.analysis.scorers import calculate_post_score; r = calculate_post_score({'raw_text': 'Test post with some content.', 'avg_sentence_length': 10.0, 'sentence_length_std': 5.0}, {}); assert 'burstiness_score' in r; print(f'burstiness in score: {r[\"burstiness_score\"]}')"` from bc-rao-api directory.

Run `cd bc-rao-api && python -m pytest tests/ -x -q` — all tests must pass.
  </verify>
  <done>
calculate_burstiness_score() implemented using CV formula with human-like (0.5-0.8) and AI-like (0.2-0.4) baselines. calculate_post_score() includes burstiness at 0.15 weight, return dict includes burstiness_score key. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. All test suites pass: `cd bc-rao-api && python -m pytest tests/ -x -q`
2. extract_community_style() returns imperfections dict with 4 metrics
3. EMPTY_STYLE_GUIDE includes opinion_landscape and imperfection_profile
4. calculate_burstiness_score() correctly scores human-like CV high, AI-like CV low
5. calculate_post_score() includes burstiness_score in return dict
6. All new fields use .get() defaults — backward compatible with existing data
</verification>

<success_criteria>
- Style extractor captures fragment_ratio, parenthetical_frequency, self_correction_rate, dash_interruption_rate
- Style guide generator prompts for opinion_landscape (loved/hated tools, controversial takes, tribal knowledge) and imperfection_profile
- Burstiness score: CV 0.5-0.8 scores 6-10 (human-like), CV 0.2-0.4 scores 0-6 (AI-like)
- All new fields gracefully degrade when data is missing
- Existing tests pass, new tests cover new functionality
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-draft-generation-anti-ai-optimization/04.1-03-SUMMARY.md`
</output>
