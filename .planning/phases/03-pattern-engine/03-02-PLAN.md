---
phase: 03-pattern-engine
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - bc-rao-api/app/services/analysis_service.py
  - bc-rao-api/app/workers/analysis_worker.py
  - bc-rao-api/app/workers/task_runner.py
  - bc-rao-api/app/api/v1/analysis.py
  - bc-rao-api/app/api/v1/router.py
  - bc-rao-api/app/models/analysis.py
autonomous: true

must_haves:
  truths:
    - "Analysis runs automatically after collection completes with no manual trigger"
    - "SSE progress streams analysis status: 'Analyzing post 47/230...' with step names"
    - "Community profiles are created per subreddit after analysis with ISC, tone, archetypes, forbidden patterns"
    - "Subreddits with fewer than 10 posts are blocked from profile creation with 'insufficient data' status"
    - "API endpoints return community profiles, scoring breakdowns, and forbidden pattern data"
    - "Users can add custom forbidden patterns but cannot remove system-detected ones"
  artifacts:
    - path: "bc-rao-api/app/services/analysis_service.py"
      provides: "Analysis pipeline orchestration: NLP -> scoring -> profiling -> pattern extraction"
      min_lines: 120
    - path: "bc-rao-api/app/workers/analysis_worker.py"
      provides: "Background task for analysis with progress tracking"
      min_lines: 40
    - path: "bc-rao-api/app/api/v1/analysis.py"
      provides: "REST + SSE endpoints for analysis trigger, profiles, scoring, blacklist"
      min_lines: 100
  key_links:
    - from: "bc-rao-api/app/workers/task_runner.py"
      to: "bc-rao-api/app/workers/analysis_worker.py"
      via: "run_analysis_background chained after collection success"
      pattern: "run_analysis_background"
    - from: "bc-rao-api/app/services/analysis_service.py"
      to: "bc-rao-api/app/analysis/nlp_pipeline.py"
      via: "imports analyze_posts_batch for NLP processing"
      pattern: "from app\\.analysis\\.nlp_pipeline import"
    - from: "bc-rao-api/app/services/analysis_service.py"
      to: "bc-rao-api/app/analysis/scorers.py"
      via: "imports scoring functions"
      pattern: "from app\\.analysis\\.scorers import"
    - from: "bc-rao-api/app/api/v1/analysis.py"
      to: "bc-rao-api/app/services/analysis_service.py"
      via: "service layer for all analysis operations"
      pattern: "AnalysisService"
---

<objective>
Build the analysis service that orchestrates NLP processing, scoring, and community profile creation. Create the background worker with auto-trigger from collection completion, SSE progress tracking, and all REST API endpoints for profiles, scoring breakdowns, and forbidden patterns.

Purpose: This connects the NLP pipeline (Plan 01) to the database and API layer, making analysis results accessible to the frontend. The auto-trigger ensures seamless pipeline flow (collect -> analyze) per user decision.

Output: Complete backend for Module 2 â€” analysis service, worker, and 7 API endpoints.
</objective>

<execution_context>
@C:\Users\quena\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\quena\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-pattern-engine/03-RESEARCH.md
@.planning/phases/03-pattern-engine/03-CONTEXT.md
@.planning/phases/03-pattern-engine/03-01-SUMMARY.md
@bc-rao-api/app/workers/task_runner.py
@bc-rao-api/app/workers/collection_worker.py
@bc-rao-api/app/api/v1/collection.py
@bc-rao-api/app/api/v1/router.py
@bc-rao-api/app/models/raw_posts.py
@bc-rao-api/app/models/analysis.py
@bc-rao-api/app/integrations/supabase_client.py
@bc-rao-api/app/dependencies.py
@bc-rao-api/app/utils/errors.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Analysis service + background worker with auto-trigger</name>
  <files>
    bc-rao-api/app/services/analysis_service.py
    bc-rao-api/app/workers/analysis_worker.py
    bc-rao-api/app/workers/task_runner.py
  </files>
  <action>
    **Create `bc-rao-api/app/services/analysis_service.py`:**

    Class `AnalysisService` with methods:

    1. `async run_analysis(campaign_id: str, force_refresh: bool = False, progress_callback=None) -> AnalysisResult`:
       Main orchestration method. Steps:
       a. Fetch all raw_posts for campaign from Supabase (select id, raw_text, subreddit, comment_count, upvote_ratio, archetype, success_score)
       b. If not force_refresh, check if community_profiles already exist for this campaign. If yes, return early with status="exists".
       c. Group posts by subreddit
       d. For each subreddit:
          - If post count < 10: skip with warning "Insufficient data for r/{subreddit}: {count} posts (need 10+)"
          - Run NLP analysis via `analyze_posts_batch([post.raw_text for post in posts])`
          - Emit progress: state="nlp_analysis", current=processed_count, total=total_posts, status=f"Analyzing post {n}/{total}..."
          - Calculate post scores via `calculate_post_score()` for each post, using subreddit averages
          - Emit progress: state="scoring", status=f"Scoring posts for r/{subreddit}..."
          - Update raw_posts table: set rhythm_metadata JSONB with NLP results, update success_score with calculated total_score
          - Calculate ISC score via `calculate_isc_score()`
          - Extract forbidden patterns via `extract_forbidden_patterns()`
          - Build community profile and upsert into community_profiles table
          - Emit progress: state="profiling", status=f"Building profile for r/{subreddit}..."
       e. Return AnalysisResult with counts

       The community_profiles upsert should use Supabase `.upsert()` with `on_conflict="campaign_id,subreddit"` to handle re-analysis.

       Community profile data to store:
       - campaign_id, subreddit
       - isc_score from calculate_isc_score()
       - avg_sentence_length: mean across all posts
       - dominant_tone: most common tone ("positive"/"negative"/"neutral")
       - formality_level: mean formality_score across posts
       - top_success_hooks: top 5 posts by score, extract first sentence as "hook" (JSONB array of strings)
       - forbidden_patterns: JSONB from pattern_extractor with {by_category, detected_patterns}
       - archetype_distribution: count by archetype {"Journey": N, "ProblemSolution": N, "Feedback": N, "Unclassified": N}
       - sample_size: total post count for subreddit

    2. `async get_community_profile(campaign_id: str, subreddit: str) -> dict`:
       Fetch single community profile from Supabase by campaign_id + subreddit.
       Add computed `isc_tier` field using `isc_to_tier()`.
       Raise AppError(RESOURCE_NOT_FOUND) if not found.

    3. `async get_community_profiles(campaign_id: str) -> list[dict]`:
       Fetch all community profiles for a campaign. Add isc_tier to each.

    4. `async get_scoring_breakdown(post_id: str) -> dict`:
       Fetch single post from raw_posts, extract rhythm_metadata JSONB.
       Re-calculate scoring breakdown from stored NLP data + community averages.
       Fetch the post's community profile for the subreddit to get community averages.
       Use `check_post_penalties()` to get penalty_phrases for inline highlighting.
       Return PostScoreBreakdown.

    5. `async get_analyzed_posts(campaign_id: str, subreddit: str = None, sort_by: str = "total_score", sort_dir: str = "desc", page: int = 1, per_page: int = 20) -> dict`:
       Fetch posts with analysis data. Filter by subreddit if provided.
       Sort by total_score (from rhythm_metadata->total_score), vulnerability, rhythm, etc.
       Return paginated list with scoring data embedded.

    6. `async get_forbidden_patterns(campaign_id: str, subreddit: str = None) -> dict`:
       Aggregate forbidden patterns from community_profiles.
       If subreddit provided, filter to that subreddit.
       Also fetch custom patterns from syntax_blacklist table (where is_system = false).
       Merge and return BlacklistResponse.

    7. `async add_custom_pattern(campaign_id: str, user_id: str, category: str, pattern: str, subreddit: str = None) -> dict`:
       Insert into syntax_blacklist table with is_system=False, source="user".
       Validate campaign ownership.

    **Create `bc-rao-api/app/workers/analysis_worker.py`:**
    Similar to collection_worker.py pattern. Contains `run_analysis_background()` async function:
    - Import AnalysisService
    - Create progress_callback that calls `update_task_state()` with analysis progress data
    - Run `service.run_analysis()` with progress callback
    - Update task state to SUCCESS or FAILURE

    **Modify `bc-rao-api/app/workers/task_runner.py`:**
    Add `run_analysis_background(task_id, campaign_id, force_refresh=False)` async function following the same pattern as `run_collection_background()`.

    CRITICAL: Modify `run_collection_background()` to auto-chain analysis after successful collection:
    After the `update_task_state(task_id, "SUCCESS", ...)` line, add:
    ```python
    # Auto-trigger analysis after successful collection (LOCKED user decision)
    analysis_task_id = generate_task_id()
    # Store analysis task ID in the collection success metadata so frontend can track it
    update_task_state(task_id, "SUCCESS", {
        ...existing_meta,
        "analysis_task_id": analysis_task_id
    })
    asyncio.create_task(
        run_analysis_background_task(analysis_task_id, campaign_id)
    )
    ```

    Import the analysis worker function. The collection SSE success event will now include `analysis_task_id` that the frontend can use to track analysis progress.
  </action>
  <verify>
    Run: `cd bc-rao-api && python -c "from app.services.analysis_service import AnalysisService; from app.workers.task_runner import run_analysis_background; print('Analysis service and worker import OK')"`

    Verify task_runner has auto-trigger: `cd bc-rao-api && python -c "import inspect; from app.workers.task_runner import run_collection_background; source = inspect.getsource(run_collection_background); assert 'analysis_task_id' in source, 'Auto-trigger not found'; print('Auto-trigger wiring verified')"`
  </verify>
  <done>
    AnalysisService orchestrates NLP -> scoring -> profiling pipeline with progress callbacks. Analysis worker runs as background task with Redis state tracking. Collection completion auto-chains analysis with analysis_task_id in success metadata.
  </done>
</task>

<task type="auto">
  <name>Task 2: FastAPI analysis endpoints + router registration</name>
  <files>
    bc-rao-api/app/api/v1/analysis.py
    bc-rao-api/app/api/v1/router.py
  </files>
  <action>
    **Create `bc-rao-api/app/api/v1/analysis.py`:**

    Router with `prefix="/analysis"`, `tags=["analysis"]`.

    Endpoints per spec:

    1. `POST /campaigns/{campaign_id}/analyze` (202 Accepted):
       - Auth required (get_current_user dependency)
       - Body: `{ force_refresh?: boolean }` (optional, default False)
       - Validate campaign ownership
       - Generate task_id, launch `run_analysis_background()` via asyncio.create_task
       - Return `{ job_id: task_id, status: "started" }`
       - Note: This is the manual trigger endpoint. Auto-trigger happens via collection completion. This endpoint exists for force-refresh and manual re-analysis.

    2. `GET /analysis/{task_id}/progress` (SSE stream):
       - No auth (task_id is bearer token, same pattern as collection SSE)
       - Stream progress events from Redis task state every 500ms
       - Events: pending, progress (with current/total/status/current_step), complete, error
       - Follow exact same SSE pattern as collection endpoint in collection.py
       - Copy the StreamingResponse + async generator pattern from collection.py

    3. `GET /campaigns/{campaign_id}/community-profile` (200):
       - Auth required
       - Query param: `?subreddit=SaaS` (required)
       - Return CommunityProfileResponse with isc_tier computed
       - 404 if profile not found

    4. `GET /campaigns/{campaign_id}/community-profiles` (200):
       - Auth required
       - Return CommunityProfileListResponse (all profiles for campaign)
       - Used for comparison table

    5. `GET /campaigns/{campaign_id}/scoring-breakdown` (200):
       - Auth required
       - Query param: `?post_id=<uuid>` (required)
       - Return PostScoreBreakdown with penalty_phrases for inline highlighting

    6. `GET /campaigns/{campaign_id}/analyzed-posts` (200):
       - Auth required
       - Query params: subreddit (optional), sort_by (default "total_score"), sort_dir (default "desc"), page (default 1), per_page (default 20)
       - Return paginated posts with analysis scores

    7. `GET /campaigns/{campaign_id}/forbidden-patterns` (200):
       - Auth required
       - Query param: subreddit (optional filter)
       - Return BlacklistResponse with patterns grouped by category and counts

    8. `POST /campaigns/{campaign_id}/forbidden-patterns` (201):
       - Auth required
       - Body: `{ category: str, pattern: str, subreddit?: str }`
       - Add custom forbidden pattern
       - Return created pattern entry

    **Modify `bc-rao-api/app/api/v1/router.py`:**
    Add import: `from app.api.v1 import analysis`
    Add: `router.include_router(analysis.router, tags=["analysis"])` after the collection router line.
  </action>
  <verify>
    Run: `cd bc-rao-api && python -c "from app.api.v1.analysis import router; print(f'Analysis router registered with {len(router.routes)} routes')"`

    Verify router registration: `cd bc-rao-api && python -c "from app.api.v1.router import router; routes = [r.path for r in router.routes if hasattr(r, 'path')]; print('Analysis routes:', [r for r in routes if 'analy' in r or 'community' in r or 'scoring' in r or 'forbidden' in r])"`

    Run build check: `cd bc-rao-api && python -c "from app.main import app; print(f'FastAPI app loaded with {len(app.routes)} routes')"`
  </verify>
  <done>
    8 analysis endpoints registered: manual trigger with SSE progress, community profile (single + list), scoring breakdown, analyzed posts with sorting/filtering, forbidden patterns CRUD. Router registered in v1 API. SSE follows established collection pattern.
  </done>
</task>

</tasks>

<verification>
- All analysis endpoints accessible (import app.main succeeds)
- SSE progress endpoint follows same pattern as collection SSE
- Auto-trigger chains analysis after collection success
- Community profile creation blocked for subreddits with < 10 posts
- Custom forbidden patterns can be added (is_system=False)
- System-detected patterns cannot be removed
</verification>

<success_criteria>
- AnalysisService orchestrates full pipeline: NLP -> scoring -> profiling -> patterns
- Collection completion auto-triggers analysis with chained task_id
- 8 API endpoints registered and importable
- SSE progress streaming works for analysis tasks
- Community profiles stored in Supabase with ISC scores
- Forbidden patterns aggregated by category with counts
</success_criteria>

<output>
After completion, create `.planning/phases/03-pattern-engine/03-02-SUMMARY.md`
</output>
