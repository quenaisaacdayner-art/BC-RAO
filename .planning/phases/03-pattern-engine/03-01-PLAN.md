---
phase: 03-pattern-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - bc-rao-api/app/analysis/__init__.py
  - bc-rao-api/app/analysis/nlp_pipeline.py
  - bc-rao-api/app/analysis/scorers.py
  - bc-rao-api/app/analysis/pattern_extractor.py
  - bc-rao-api/app/models/analysis.py
  - bc-rao-api/pyproject.toml
autonomous: true

must_haves:
  truths:
    - "SpaCy pipeline processes post text and extracts formality score, tone, avg sentence length, vocabulary complexity"
    - "Scoring functions calculate ISC, vulnerability weight, rhythm adherence, marketing jargon penalty, link density penalty"
    - "Pattern extractor detects forbidden patterns and categorizes them into Promotional, Self-referential, Link patterns, etc."
    - "All NLP runs locally with zero external API cost"
  artifacts:
    - path: "bc-rao-api/app/analysis/nlp_pipeline.py"
      provides: "SpaCy custom pipeline with formality, tone, rhythm components"
      min_lines: 80
    - path: "bc-rao-api/app/analysis/scorers.py"
      provides: "ISC scoring, post success scoring, vulnerability/rhythm/penalty calculations"
      min_lines: 100
    - path: "bc-rao-api/app/analysis/pattern_extractor.py"
      provides: "Forbidden pattern detection with category classification"
      min_lines: 60
    - path: "bc-rao-api/app/models/analysis.py"
      provides: "Pydantic models for analysis results, community profiles, scoring breakdowns"
      min_lines: 40
  key_links:
    - from: "bc-rao-api/app/analysis/nlp_pipeline.py"
      to: "spacy, textstat, vaderSentiment"
      via: "custom @Language.component decorators"
      pattern: "@Language\\.component"
    - from: "bc-rao-api/app/analysis/scorers.py"
      to: "bc-rao-api/app/analysis/nlp_pipeline.py"
      via: "imports pipeline results for scoring"
      pattern: "from app\\.analysis\\.nlp_pipeline import"
---

<objective>
Build the SpaCy NLP pipeline with custom components for formality/tone/rhythm analysis, post success scoring algorithms, and forbidden pattern extraction with categorization.

Purpose: This is the computational core of Module 2 (Pattern Engine). All NLP analysis runs locally via SpaCy with zero API cost. The pipeline processes collected posts to extract linguistic features that feed into community profiling and post scoring.

Output: Python modules for NLP analysis, scoring calculations, and pattern extraction that the analysis service (Plan 02) will orchestrate.
</objective>

<execution_context>
@C:\Users\quena\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\quena\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-pattern-engine/03-RESEARCH.md
@.planning/phases/03-pattern-engine/03-CONTEXT.md
@bc-rao-api/app/models/raw_posts.py
@bc-rao-api/pyproject.toml
@bc-rao-api/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: SpaCy NLP pipeline with custom components + Pydantic models</name>
  <files>
    bc-rao-api/app/analysis/__init__.py
    bc-rao-api/app/analysis/nlp_pipeline.py
    bc-rao-api/app/models/analysis.py
    bc-rao-api/pyproject.toml
  </files>
  <action>
    **Install dependencies:** Add spacy, textstat, vaderSentiment to pyproject.toml dependencies:
    - `"spacy>=3.8.0"`
    - `"textstat>=0.7.12"`
    - `"vaderSentiment>=3.3.2"`

    Note: The en_core_web_md model download (`python -m spacy download en_core_web_md`) is a runtime requirement. Add a comment in nlp_pipeline.py documenting this.

    **Create `bc-rao-api/app/analysis/__init__.py`:** Empty init file.

    **Create `bc-rao-api/app/analysis/nlp_pipeline.py`:**
    - Load SpaCy `en_core_web_md` model at module level with `disable=["ner"]` (NER not needed, saves memory)
    - Call `textstat.set_lang('en')` at module level per research pitfall 6
    - Register custom Doc extensions using `Doc.set_extension(..., force=True)`:
      - `formality_score` (float): Average of Flesch-Kincaid grade + Gunning FOG
      - `tone` (str): "positive", "negative", or "neutral" from VADER compound score
      - `tone_compound` (float): Raw VADER compound score for aggregation
      - `avg_sentence_length` (float): Average token count per sentence
      - `sentence_length_std` (float): Standard deviation of sentence lengths (rhythm consistency)
      - `vocabulary_complexity` (float): Ratio of unique lemmas to total tokens (type-token ratio)
      - `num_sentences` (int): Total sentence count

    - Register three custom pipeline components via `@Language.component`:

      1. `formality_scorer`: Uses textstat `flesch_kincaid_grade()` and `gunning_fog()`. Average = formality_score. Skip texts < 20 chars. Handle textstat errors with try/except (some very short texts cause division by zero).

      2. `tone_classifier`: Uses `SentimentIntensityAnalyzer.polarity_scores()`. Classify compound >= 0.05 as "positive", <= -0.05 as "negative", else "neutral". Store both tone string and tone_compound float.

      3. `rhythm_analyzer`: Calculate per-sentence token lengths from `doc.sents`. Compute avg_sentence_length and sentence_length_std (using statistics.stdev or manual calc, handle single-sentence case with std=0). Also compute vocabulary_complexity as len(unique_lemmas) / len(doc) for tokens where `token.is_alpha`.

    - Add all three components to pipeline with `nlp.add_pipe(..., last=True)`

    - Export function `get_nlp_pipeline()` that returns the configured nlp instance (singleton pattern)

    - Export function `analyze_posts_batch(texts: list[str], batch_size: int = 100) -> list[dict]` that:
      - Uses `nlp.pipe(texts, batch_size=batch_size)` (NEVER use n_process inside workers per research pitfall 2)
      - Returns list of dicts with keys: formality_score, tone, tone_compound, avg_sentence_length, sentence_length_std, vocabulary_complexity, num_sentences
      - Handles empty/None texts gracefully (return default values)

    **Create `bc-rao-api/app/models/analysis.py`:**
    - `NLPResult(BaseModel)`: formality_score (Optional[float]), tone (Optional[str]), tone_compound (Optional[float]), avg_sentence_length (Optional[float]), sentence_length_std (Optional[float]), vocabulary_complexity (Optional[float]), num_sentences (Optional[int])

    - `PostScoreBreakdown(BaseModel)`: post_id (UUID), vulnerability_weight (float), rhythm_adherence (float), formality_match (float), marketing_jargon_penalty (float), link_density_penalty (float), total_score (float), penalty_phrases (list of dict with keys: phrase, severity, category)

    - `CommunityProfileResponse(BaseModel)`: id (UUID), campaign_id (UUID), subreddit (str), isc_score (float), isc_tier (str — computed from score), avg_sentence_length (Optional[float]), dominant_tone (Optional[str]), formality_level (Optional[float]), top_success_hooks (list), forbidden_patterns (list), archetype_distribution (dict), sample_size (int), last_analyzed_at (datetime)

    - `CommunityProfileListResponse(BaseModel)`: profiles (list[CommunityProfileResponse])

    - `AnalysisProgress(BaseModel)`: state (str), current (int), total (int), status (str), current_step (str — e.g. "nlp_analysis", "scoring", "profiling")

    - `AnalysisResult(BaseModel)`: status (str), posts_analyzed (int), profiles_created (int), errors (list[str])

    - `ForbiddenPatternEntry(BaseModel)`: id (UUID), category (str), pattern (str), subreddit (Optional[str] — None means global), is_system (bool — True for auto-detected, False for user-added), count (int)

    - `BlacklistResponse(BaseModel)`: patterns (list[ForbiddenPatternEntry]), total (int), categories (dict[str, int] — category name to count)

    - Helper function `isc_to_tier(score: float) -> str` returning: score < 3 = "Low Sensitivity", 3-5 = "Moderate Sensitivity", 5-7 = "High Sensitivity", 7+ = "Very High Sensitivity"
  </action>
  <verify>
    Run: `cd bc-rao-api && python -c "from app.analysis.nlp_pipeline import get_nlp_pipeline, analyze_posts_batch; from app.models.analysis import PostScoreBreakdown, CommunityProfileResponse, AnalysisProgress; print('NLP pipeline and models import OK')"`

    Verify pipeline processes text: `python -c "from app.analysis.nlp_pipeline import analyze_posts_batch; results = analyze_posts_batch(['This is a test post about building a SaaS product.']); print(results)"`
  </verify>
  <done>
    SpaCy pipeline loads en_core_web_md with 3 custom components (formality_scorer, tone_classifier, rhythm_analyzer). analyze_posts_batch() processes texts in batches and returns NLP metrics. All Pydantic models for analysis responses are defined.
  </done>
</task>

<task type="auto">
  <name>Task 2: Post success scoring + ISC calculator + forbidden pattern extractor</name>
  <files>
    bc-rao-api/app/analysis/scorers.py
    bc-rao-api/app/analysis/pattern_extractor.py
  </files>
  <action>
    **Create `bc-rao-api/app/analysis/scorers.py`:**

    Implement post-level scoring with these functions:

    1. `calculate_post_score(post_data: dict, community_avg: dict) -> dict`:
       Takes a post's NLP results + engagement data and a community average profile.
       Returns dict with: vulnerability_weight, rhythm_adherence, formality_match, marketing_jargon_penalty, link_density_penalty, total_score, penalty_phrases.

       Component calculations:
       - `vulnerability_weight` (0-10): Score based on presence of personal pronouns (I, my, we), emotional language markers, question marks, storytelling signals. Higher = more vulnerable/authentic. Use compiled regex patterns.
       - `rhythm_adherence` (0-10): How closely post's sentence length distribution matches community average. Calculate as 10 - abs(post_avg_sentence_length - community_avg_sentence_length) clamped to 0-10. Also factor in sentence_length_std similarity.
       - `formality_match` (0-10): How closely post's formality matches community. 10 - abs(post_formality - community_formality) * 2, clamped 0-10.
       - `marketing_jargon_penalty` (0-10, higher = worse): Detect jargon patterns: synerg*, leverage, paradigm, disrupt*, innovate*, game-changer, thought leader, best-in-class, reach out, circle back, touch base, revolutionary, cutting-edge, scalable, ROI, growth hack. Count matches, scale: 0 matches = 0, 1 = 3, 2 = 5, 3+ = 8-10.
       - `link_density_penalty` (0-10, higher = worse): Count URLs in text. 0 links = 0, 1 link = 3, 2 links = 6, 3+ = 9.
       - `penalty_phrases`: List of {phrase, severity, category} for each detected jargon/link. severity = "high" for 3+ matches, "medium" for 2, "low" for 1. category = "Promotional", "Link patterns", etc.

       Total score formula:
       ```
       total = (vulnerability_weight * 0.25 + rhythm_adherence * 0.25 + formality_match * 0.2) - (marketing_jargon_penalty * 0.15 + link_density_penalty * 0.15)
       ```
       Clamp total to 0-10.

    2. `calculate_isc_score(posts_data: list[dict]) -> float`:
       Takes aggregated NLP + engagement data for all posts in a subreddit.
       ISC (Intrinsic Sensitivity Coefficient) measures how sensitive a community is to marketing content. Scale 1.0 to 10.0.

       Factors (per research ISC algorithm):
       - `jargon_sensitivity` (0-10): Ratio of low-scoring posts with jargon vs high-scoring. Use regex patterns from marketing_jargon_penalty. If no jargon posts found, default 5.0.
       - `link_sensitivity` (0-10): Ratio of low-scoring posts with links vs high-scoring.
       - `vulnerability_preference` (0-10): Average vulnerability_weight of top-scoring posts vs bottom. Higher = community rewards vulnerability.
       - `depth_correlation` (0-10): Whether higher comment_count correlates with authentic-sounding posts (higher formality_match + vulnerability).

       Weighted average: jargon * 0.3 + link * 0.2 + vulnerability * 0.3 + depth * 0.2
       Round to 1 decimal place.

       Raise ValueError if len(posts_data) < 10 (insufficient data threshold per Claude's discretion).

    **Create `bc-rao-api/app/analysis/pattern_extractor.py`:**

    1. `PATTERN_CATEGORIES` dict mapping category names to compiled regex lists:
       - "Promotional": affiliate links, discount codes, coupon patterns, "check out my", "I made a", promo language
       - "Self-referential": excessive self-promotion, "my product", "my tool", "I built", "my startup", company name drops
       - "Link patterns": multiple URLs, URL shorteners (bit.ly, tinyurl), Amazon affiliate, UTM parameters
       - "Low-effort": very short posts (< 50 chars), copy-paste templates, generic "thoughts?" endings
       - "Spam indicators": excessive emojis, ALL CAPS sections, repetitive phrases, wall-of-text without paragraphs
       - "Off-topic": completely unrelated content signals (hard to detect, keep minimal)

    2. `extract_forbidden_patterns(texts: list[str]) -> dict`:
       Takes list of post texts, scans each for pattern matches.
       Returns dict with:
       - `by_category`: {category: count} — how many posts matched each category
       - `detected_patterns`: list of {category, pattern_description, match_count, severity}
       severity: "high" if > 20% of posts match, "medium" if 10-20%, "low" if < 10%

    3. `check_post_penalties(text: str) -> list[dict]`:
       Check single post text against all pattern categories.
       Return list of {phrase: str, severity: str, category: str} for inline highlighting.
       The `phrase` should be the actual matched text substring (not the regex pattern).
       Use `re.finditer()` to get match spans and extract actual text.
  </action>
  <verify>
    Run: `cd bc-rao-api && python -c "from app.analysis.scorers import calculate_post_score, calculate_isc_score; from app.analysis.pattern_extractor import extract_forbidden_patterns, check_post_penalties; print('Scorers and pattern extractor import OK')"`

    Test scoring: `python -c "
from app.analysis.scorers import calculate_post_score
score = calculate_post_score(
    {'raw_text': 'I struggled with this exact problem for months. Here is what finally worked for me.', 'formality_score': 5.0, 'tone': 'positive', 'avg_sentence_length': 8.0, 'comment_count': 15, 'upvote_ratio': 0.95},
    {'avg_sentence_length': 10.0, 'formality_level': 5.5}
)
print(f'Total score: {score[\"total_score\"]}, Vulnerability: {score[\"vulnerability_weight\"]}')
"`

    Test pattern extraction: `python -c "
from app.analysis.pattern_extractor import check_post_penalties
penalties = check_post_penalties('Check out my revolutionary SaaS tool at https://example.com - it is a game-changer for growth hacking!')
print(f'Penalties found: {len(penalties)}')
for p in penalties: print(f'  {p[\"category\"]}: {p[\"phrase\"]}')
"`
  </verify>
  <done>
    calculate_post_score returns vulnerability_weight, rhythm_adherence, formality_match, jargon penalty, link penalty, total score, and penalty_phrases for inline highlighting. calculate_isc_score computes community sensitivity from aggregated post data. extract_forbidden_patterns categorizes detected patterns by category with counts. check_post_penalties returns per-post penalty phrases for linter-style highlighting.
  </done>
</task>

</tasks>

<verification>
- All analysis modules import without errors
- SpaCy pipeline processes sample text and returns NLP metrics
- Scoring functions return valid 0-10 scores
- Pattern extractor detects promotional/link/jargon patterns
- No external API calls in any analysis code (zero cost)
</verification>

<success_criteria>
- SpaCy pipeline with 3 custom components (formality, tone, rhythm) processes text batches
- Post scoring calculates 5 factors + total score with penalty phrase extraction
- ISC scoring aggregates community sensitivity from post data
- Pattern extractor categorizes forbidden patterns into 6 categories
- All Pydantic models defined for API responses
</success_criteria>

<output>
After completion, create `.planning/phases/03-pattern-engine/03-01-SUMMARY.md`
</output>
