---
phase: 02-collection-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - bc-rao-api/app/integrations/apify_client.py
  - bc-rao-api/app/services/collection_service.py
  - bc-rao-api/app/services/regex_filter.py
  - bc-rao-api/app/models/raw_posts.py
  - bc-rao-api/pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Apify client can execute Reddit scraper actor and return post data"
    - "Regex pre-filter removes ~80% of low-quality posts before AI processing"
    - "Collection service orchestrates the full pipeline: scrape -> filter -> classify -> store"
    - "Top 10% of filtered posts are classified into archetypes via InferenceClient"
    - "Posts are stored in raw_posts table with deduplication on reddit_post_id"
  artifacts:
    - path: "bc-rao-api/app/integrations/apify_client.py"
      provides: "Apify SDK wrapper for Reddit scraping"
      min_lines: 40
    - path: "bc-rao-api/app/services/regex_filter.py"
      provides: "Regex pre-filter with compiled patterns for post quality scoring"
      min_lines: 50
    - path: "bc-rao-api/app/services/collection_service.py"
      provides: "Collection pipeline orchestration: scrape, filter, classify, store"
      min_lines: 100
    - path: "bc-rao-api/app/models/raw_posts.py"
      provides: "Pydantic models for raw_posts CRUD and API responses"
      min_lines: 40
  key_links:
    - from: "bc-rao-api/app/services/collection_service.py"
      to: "bc-rao-api/app/integrations/apify_client.py"
      via: "import and call scrape_subreddit"
      pattern: "apify_client.*scrape"
    - from: "bc-rao-api/app/services/collection_service.py"
      to: "bc-rao-api/app/services/regex_filter.py"
      via: "import and call filter function"
      pattern: "regex_filter.*filter"
    - from: "bc-rao-api/app/services/collection_service.py"
      to: "bc-rao-api/app/inference/client.py"
      via: "InferenceClient for archetype classification"
      pattern: "InferenceClient.*classify"
    - from: "bc-rao-api/app/services/collection_service.py"
      to: "bc-rao-api/app/integrations/supabase_client.py"
      via: "Supabase insert with ON CONFLICT for dedup"
      pattern: "supabase.*raw_posts"
---

<objective>
Build the backend collection pipeline core: Apify integration for Reddit scraping, regex pre-filter for quality scoring, LLM archetype classification, and database storage with deduplication.

Purpose: This is the data engine behind Phase 2. Without the pipeline, there is nothing to display in the UI. Every other plan in this phase depends on this foundation.
Output: Four backend modules (apify_client, regex_filter, collection_service, raw_posts models) that together can scrape Reddit, filter posts, classify archetypes, and store results.
</objective>

<execution_context>
@C:\Users\quena\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\quena\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-collection-pipeline/02-CONTEXT.md
@.planning/phases/02-collection-pipeline/02-RESEARCH.md
@bc-rao-api/app/config.py
@bc-rao-api/app/integrations/supabase_client.py
@bc-rao-api/app/inference/client.py
@bc-rao-api/app/inference/router.py
@bc-rao-api/app/models/campaign.py
@bc-rao-api/app/utils/errors.py
@migrations/001_initial_schema.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Apify client wrapper + regex pre-filter module + raw_posts Pydantic models</name>
  <files>
    bc-rao-api/app/integrations/apify_client.py
    bc-rao-api/app/services/regex_filter.py
    bc-rao-api/app/models/raw_posts.py
    bc-rao-api/pyproject.toml
  </files>
  <action>
    1. Install apify-client: Add `apify-client>=1.6.0` to pyproject.toml dependencies. Run `pip install apify-client`.

    2. Create `bc-rao-api/app/integrations/apify_client.py`:
       - Import ApifyClient from apify_client
       - Create function `scrape_subreddit(subreddit: str, keywords: list[str], max_posts: int = 100) -> list[dict]`
       - Use `settings.APIFY_API_TOKEN` for auth and `settings.APIFY_REDDIT_ACTOR_ID` for actor selection (both already in config.py)
       - Actor input: subreddits=[subreddit], searchTerms=keywords, maxPosts=max_posts, sort="hot", timeFilter="month"
       - Call `client.actor(actor_id).call(run_input=actor_input)` then `client.dataset(run["defaultDatasetId"]).list_items().items`
       - Return list of raw post dicts
       - Wrap in try/except, raise AppError(EXTERNAL_SERVICE_ERROR) on failure with subreddit name in details
       - If APIFY_API_TOKEN is empty, raise AppError with clear message "Apify API token not configured"

    3. Create `bc-rao-api/app/services/regex_filter.py`:
       - Pre-compile regex patterns at module level using `re.compile()`
       - Quality signals to KEEP (positive patterns): personal pronouns (I/my/we), question marks, emotional language (frustrated/excited/helped/struggle), specific numbers/metrics, storytelling markers (then/after/finally/realized)
       - Quality signals to REJECT (negative patterns): pure link dumps (http only posts), very short posts (< 50 chars), bot/automated content markers ([removed], [deleted]), pure promotional language without story (buy now, use code, limited time)
       - Function `filter_posts(posts: list[dict], campaign_keywords: list[str]) -> list[dict]`:
         - For each post, compute a relevance_score (0-10) based on: keyword match count, positive pattern matches, post length, engagement ratio (upvotes/comments)
         - Reject posts matching negative patterns (return empty for those)
         - Return posts sorted by relevance_score descending
         - Target: ~80% rejection rate (keep top ~20%)
       - Function `select_top_for_classification(filtered_posts: list[dict], top_percent: float = 0.1) -> list[dict]`:
         - Returns top N% of filtered posts by relevance_score for LLM classification
       - Use non-capturing groups `(?:...)` and avoid nested quantifiers for performance

    4. Create `bc-rao-api/app/models/raw_posts.py`:
       - RawPostCreate: campaign_id (UUID), subreddit (str), reddit_post_id (str), reddit_url (str, optional), author (str, optional), author_karma (int, optional), title (str), raw_text (str), comment_count (int, default 0), upvote_ratio (float, optional), archetype (str, default "Unclassified"), success_score (float, optional), reddit_created_at (datetime, optional)
       - RawPostResponse: extends RawPostCreate with id (UUID), user_id (UUID), is_ai_processed (bool), collected_at (datetime)
       - RawPostWithMeta: extends RawPostResponse (used for detail modal) - same fields, nothing extra needed since schema covers it
       - RawPostListResponse: posts (list[RawPostResponse]), total (int), page (int), per_page (int)
       - CollectionProgress: state (str), scraped (int), filtered (int), classified (int), current_step (int), total_steps (int), current_subreddit (str, optional), errors (list[str])
       - CollectionResult: status (str: "complete" | "partial"), scraped (int), filtered (int), classified (int), errors (list[str])
  </action>
  <verify>
    Run from bc-rao-api directory:
    - `python -c "from app.integrations.apify_client import scrape_subreddit; print('apify_client OK')"` succeeds
    - `python -c "from app.services.regex_filter import filter_posts, select_top_for_classification; print('regex_filter OK')"` succeeds
    - `python -c "from app.models.raw_posts import RawPostCreate, RawPostResponse, CollectionProgress; print('models OK')"` succeeds
    - `pip install apify-client` completes without error
  </verify>
  <done>
    Apify client wrapper importable and configured with settings. Regex filter module importable with compiled patterns. Raw posts Pydantic models importable with all fields matching database schema. apify-client dependency installed and in pyproject.toml.
  </done>
</task>

<task type="auto">
  <name>Task 2: Collection service - full pipeline orchestration</name>
  <files>
    bc-rao-api/app/services/collection_service.py
  </files>
  <action>
    Create `bc-rao-api/app/services/collection_service.py` with class `CollectionService`:

    1. `__init__(self)`: Initialize with supabase client import

    2. `async def run_collection(self, campaign_id: str, user_id: str, plan: str, progress_callback=None) -> CollectionResult`:
       - Fetch campaign from Supabase to get target_subreddits and keywords
       - If campaign not found or not owned by user_id, raise AppError(NOT_FOUND)
       - Initialize counters: scraped_total, filtered_total, classified_total, errors list
       - For each subreddit in campaign.target_subreddits:
         a. Call progress_callback if provided with current state (step, subreddit, counts)
         b. Try: scrape_subreddit(subreddit, campaign.keywords) via apify_client
         c. On failure: append to errors list, continue to next subreddit (partial failure per user decision)
         d. scraped_total += len(scraped_posts)
         e. Apply regex_filter.filter_posts(scraped_posts, campaign.keywords)
         f. filtered_total += len(filtered_posts)
         g. Select top 10% via select_top_for_classification(filtered_posts)
         h. For each top post: classify archetype via InferenceClient("classify_archetype")
         i. classified_total += len(classified_posts)
         j. Store all posts to raw_posts table via _store_posts()
       - Return CollectionResult with final counts and errors

    3. `async def _classify_post(self, post: dict, user_id: str, plan: str, campaign_id: str) -> dict`:
       - Build classification prompt: "Classify this Reddit post into exactly one archetype: Journey (personal story/experience), ProblemSolution (asking for or providing solutions), or Feedback (opinions/reviews/recommendations). Also score its success potential from 0-10 based on engagement signals. Post title: {title}. Post text: {text[:500]}. Respond with JSON: {\"archetype\": \"Journey|ProblemSolution|Feedback\", \"success_score\": 7.5}"
       - Call InferenceClient("classify_archetype").call(prompt, user_id, plan, campaign_id)
       - Parse JSON response, extract archetype and success_score
       - On parse failure: default to archetype="Unclassified", success_score=5.0
       - Return post dict updated with archetype and success_score, is_ai_processed=True

    4. `async def _store_posts(self, posts: list[dict], campaign_id: str, user_id: str) -> int`:
       - Use Supabase client to insert posts into raw_posts table
       - Map post dict fields to raw_posts columns (reddit_post_id, title, raw_text, subreddit, author, comment_count, upvote_ratio, archetype, success_score, reddit_url, reddit_created_at)
       - Use upsert with `on_conflict="campaign_id,reddit_post_id"` and `ignore_duplicates=True` for deduplication
       - Return count of newly inserted posts

    5. `async def get_posts(self, campaign_id: str, user_id: str, archetype: str = None, subreddit: str = None, min_score: float = None, max_score: float = None, page: int = 1, per_page: int = 20) -> RawPostListResponse`:
       - Query raw_posts filtered by campaign_id and user_id (RLS handles user_id)
       - Apply optional filters: archetype, subreddit, min_score (gte), max_score (lte)
       - Order by success_score DESC, then collected_at DESC
       - Apply pagination: offset = (page - 1) * per_page, limit = per_page
       - Return RawPostListResponse with posts, total count, page, per_page

    6. `async def get_post_detail(self, post_id: str, user_id: str) -> RawPostResponse`:
       - Fetch single post by ID from raw_posts, verify user ownership via RLS
       - Return RawPostResponse or raise AppError(NOT_FOUND)

    7. `async def get_collection_stats(self, campaign_id: str, user_id: str) -> dict`:
       - Return counts: total posts, by archetype (Journey, ProblemSolution, Feedback, Unclassified), by subreddit, avg success_score
       - Single query with GROUP BY or multiple count queries
  </action>
  <verify>
    Run from bc-rao-api directory:
    - `python -c "from app.services.collection_service import CollectionService; print('CollectionService OK')"` succeeds
    - `python -c "from app.services.collection_service import CollectionService; cs = CollectionService(); print(dir(cs))"` shows run_collection, get_posts, get_post_detail, get_collection_stats methods
  </verify>
  <done>
    CollectionService fully importable with all methods. Pipeline orchestration logic connects Apify scraping -> regex filtering -> LLM classification -> Supabase storage. Partial failure handling continues with successful subreddits. Deduplication via Supabase upsert with ignore_duplicates. Post querying supports archetype, subreddit, and score range filters with pagination.
  </done>
</task>

</tasks>

<verification>
All 4 new modules import cleanly:
- `from app.integrations.apify_client import scrape_subreddit`
- `from app.services.regex_filter import filter_posts, select_top_for_classification`
- `from app.services.collection_service import CollectionService`
- `from app.models.raw_posts import RawPostCreate, RawPostResponse, CollectionProgress, CollectionResult`

apify-client package installed and listed in pyproject.toml.
</verification>

<success_criteria>
- Apify client wrapper configured with env var-based actor ID selection
- Regex filter compiles patterns at module level, scores posts, targets ~80% rejection
- Collection service orchestrates full pipeline with partial failure handling
- Pydantic models match raw_posts database schema
- Deduplication uses Supabase upsert (not application-level checks)
- All modules importable without runtime errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-collection-pipeline/02-01-SUMMARY.md`
</output>
