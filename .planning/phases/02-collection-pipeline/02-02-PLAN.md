---
phase: 02-collection-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - bc-rao-api/app/workers/collection_worker.py
  - bc-rao-api/app/api/v1/collection.py
  - bc-rao-api/app/api/v1/router.py
autonomous: true

must_haves:
  truths:
    - "Collection can be triggered via POST endpoint and runs as async Celery task"
    - "Celery worker emits PROGRESS state updates with scraped/filtered/classified counts"
    - "SSE endpoint streams real-time progress from Celery task state to clients"
    - "API returns collected posts with filtering by archetype, subreddit, and score range"
    - "Collection runs without blocking the dashboard"
  artifacts:
    - path: "bc-rao-api/app/workers/collection_worker.py"
      provides: "Celery bound task with progress tracking via update_state()"
      min_lines: 40
    - path: "bc-rao-api/app/api/v1/collection.py"
      provides: "REST endpoints: trigger collection, SSE progress, fetch posts, post detail"
      min_lines: 80
  key_links:
    - from: "bc-rao-api/app/workers/collection_worker.py"
      to: "bc-rao-api/app/services/collection_service.py"
      via: "imports and calls CollectionService.run_collection"
      pattern: "CollectionService.*run_collection"
    - from: "bc-rao-api/app/workers/collection_worker.py"
      to: "bc-rao-api/app/workers/celery_app.py"
      via: "shared_task decorator with bind=True"
      pattern: "shared_task.*bind.*True"
    - from: "bc-rao-api/app/api/v1/collection.py"
      to: "bc-rao-api/app/workers/collection_worker.py"
      via: "task.delay() to trigger async execution"
      pattern: "collect_campaign_data\\.delay"
    - from: "bc-rao-api/app/api/v1/collection.py"
      to: "celery.result.AsyncResult"
      via: "SSE endpoint polls task state"
      pattern: "AsyncResult.*state"
    - from: "bc-rao-api/app/api/v1/router.py"
      to: "bc-rao-api/app/api/v1/collection.py"
      via: "include_router registration"
      pattern: "collection\\.router"
---

<objective>
Wire the collection pipeline to Celery for async execution and expose it through FastAPI endpoints: trigger collection, stream progress via SSE, and query collected posts with filters.

Purpose: This plan connects the pipeline core (Plan 01) to the web layer, making collection triggerable from the frontend and providing real-time progress feedback.
Output: Celery worker task with progress tracking, and FastAPI endpoints for triggering, monitoring, and querying collections.
</objective>

<execution_context>
@C:\Users\quena\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\quena\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-collection-pipeline/02-CONTEXT.md
@.planning/phases/02-collection-pipeline/02-RESEARCH.md
@.planning/phases/02-collection-pipeline/02-01-SUMMARY.md
@bc-rao-api/app/workers/celery_app.py
@bc-rao-api/app/dependencies.py
@bc-rao-api/app/api/v1/router.py
@bc-rao-api/app/api/v1/campaigns.py
@bc-rao-api/app/models/raw_posts.py
@bc-rao-api/app/services/collection_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Celery collection worker with progress tracking</name>
  <files>
    bc-rao-api/app/workers/collection_worker.py
  </files>
  <action>
    Create `bc-rao-api/app/workers/collection_worker.py`:

    1. Import celery_app from app.workers.celery_app, CollectionService from app.services.collection_service
    2. Use `@celery_app.task(bind=True, name="app.workers.tasks.scraping.collect_campaign_data")` decorator
       - The name must match the routing pattern in celery_app.py: "app.workers.tasks.scraping.*" -> queue "scraping"
    3. Function signature: `def collect_campaign_data(self, campaign_id: str, user_id: str, plan: str) -> dict`
    4. Create a progress_callback function that calls `self.update_state(state='PROGRESS', meta={...})`:
       - Meta dict: scraped (int), filtered (int), classified (int), current_step (int), total_steps (int), current_subreddit (str), errors (list)
       - Guard with `if not self.request.called_directly:` to allow direct testing
    5. Instantiate CollectionService and call `run_collection(campaign_id, user_id, plan, progress_callback=progress_callback)`
       - Since CollectionService methods are async but Celery tasks are sync, use `asyncio.run()` or `asyncio.get_event_loop().run_until_complete()` to bridge
       - Import asyncio at top
    6. On success, return dict: {"status": "complete", "scraped": N, "filtered": N, "classified": N, "errors": [...]}
    7. On exception, update state to FAILURE with error details, then re-raise
    8. Set task retry policy: max_retries=0 (no auto retry per user decision), acks_late=True (already set globally)
  </action>
  <verify>
    Run from bc-rao-api directory:
    - `python -c "from app.workers.collection_worker import collect_campaign_data; print('worker OK')"` succeeds
    - `python -c "from app.workers.collection_worker import collect_campaign_data; print(collect_campaign_data.name)"` prints "app.workers.tasks.scraping.collect_campaign_data"
  </verify>
  <done>
    Celery task importable, named correctly for scraping queue routing, uses bind=True with update_state for PROGRESS tracking. Bridges async CollectionService to sync Celery task. No automatic retry.
  </done>
</task>

<task type="auto">
  <name>Task 2: FastAPI collection endpoints - trigger, SSE progress, fetch posts</name>
  <files>
    bc-rao-api/app/api/v1/collection.py
    bc-rao-api/app/api/v1/router.py
  </files>
  <action>
    1. Create `bc-rao-api/app/api/v1/collection.py` with APIRouter(prefix="/collection", tags=["collection"]):

    a. POST `/campaigns/{campaign_id}/collect` — Trigger collection
       - Requires auth via `get_current_user` dependency
       - Validates campaign exists and belongs to user (fetch from Supabase)
       - Calls `collect_campaign_data.delay(campaign_id, user_id, plan)` to queue Celery task
       - Returns 202 Accepted with `{"task_id": result.id, "status": "queued"}`
       - Does NOT enforce billing limits (deferred to Phase 6 per user decision)

    b. GET `/collection/{task_id}/progress` — SSE progress stream
       - Returns StreamingResponse with media_type="text/event-stream"
       - Headers: Cache-Control: no-cache, Connection: keep-alive, X-Accel-Buffering: no
       - Async generator function `progress_stream(task_id: str)`:
         - Create AsyncResult(task_id) from celery_app
         - While not result.ready():
           - Read result.state and result.info
           - If state == "PROGRESS": yield SSE data with meta (scraped, filtered, classified, current_step, total_steps, current_subreddit)
           - If state == "STARTED": yield SSE data with state="started"
           - If state == "PENDING": yield SSE data with state="pending"
           - await asyncio.sleep(0.5) between polls
         - On ready: If state == "SUCCESS": yield final SSE with result.result. If "FAILURE": yield error SSE
         - Yield final "event: done\ndata: {}\n\n" to signal completion
       - No auth required on SSE endpoint (task_id is unguessable UUID, acts as bearer token)

    c. GET `/campaigns/{campaign_id}/posts` — Fetch collected posts with filters
       - Requires auth via `get_current_user` dependency
       - Query params: archetype (optional str), subreddit (optional str), min_score (optional float), max_score (optional float), page (optional int, default 1), per_page (optional int, default 20)
       - Calls CollectionService.get_posts() with all params
       - Returns RawPostListResponse

    d. GET `/campaigns/{campaign_id}/posts/{post_id}` — Get post detail
       - Requires auth via `get_current_user` dependency
       - Calls CollectionService.get_post_detail()
       - Returns RawPostResponse

    e. GET `/campaigns/{campaign_id}/collection-stats` — Get collection statistics
       - Requires auth via `get_current_user` dependency
       - Calls CollectionService.get_collection_stats()
       - Returns dict with total, by_archetype, by_subreddit, avg_score

    2. Update `bc-rao-api/app/api/v1/router.py`:
       - Import collection from app.api.v1
       - Add: `router.include_router(collection.router, prefix="/collection", tags=["collection"])`
       - Final endpoint paths will be: /v1/collection/campaigns/{id}/collect, /v1/collection/{task_id}/progress, etc.
  </action>
  <verify>
    Run from bc-rao-api directory:
    - `python -c "from app.api.v1.collection import router; print([r.path for r in router.routes])"` shows all 5 endpoints
    - `python -c "from app.api.v1.router import router; print([r.path for r in router.routes])"` includes collection routes
    - `python -c "from app.main import app; print('app OK')"` succeeds (no import errors in main app)
  </verify>
  <done>
    Five collection endpoints registered: trigger (POST 202), SSE progress (GET streaming), fetch posts (GET with filters+pagination), post detail (GET), and collection stats (GET). SSE endpoint uses StreamingResponse with proper headers for NGINX compatibility. Collection router registered in v1 router. All endpoints importable without errors.
  </done>
</task>

</tasks>

<verification>
- Celery worker task importable and correctly named for scraping queue routing
- All 5 API endpoints registered and accessible via v1 router
- SSE endpoint returns StreamingResponse with text/event-stream media type
- FastAPI app starts without import errors: `python -c "from app.main import app; print('OK')"`
- POST trigger returns 202 with task_id
- GET posts supports archetype, subreddit, min_score, max_score, page, per_page params
</verification>

<success_criteria>
- Collection triggerable via POST endpoint, returns task_id
- Celery task emits PROGRESS updates with scraped/filtered/classified counters
- SSE endpoint streams progress events in real-time format
- Posts queryable with archetype, subreddit, and score filters plus pagination
- Post detail endpoint returns full post data for modal display
- Collection stats endpoint returns aggregated counts by archetype and subreddit
- No blocking of dashboard during collection (async Celery task)
</success_criteria>

<output>
After completion, create `.planning/phases/02-collection-pipeline/02-02-SUMMARY.md`
</output>
