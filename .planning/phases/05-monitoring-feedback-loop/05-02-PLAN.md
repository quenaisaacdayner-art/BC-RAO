---
phase: 05-monitoring-feedback-loop
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - bc-rao-api/app/api/v1/monitoring.py
  - bc-rao-api/app/api/v1/router.py
  - bc-rao-api/app/workers/monitoring_worker.py
  - bc-rao-api/app/workers/task_runner.py
autonomous: true

must_haves:
  truths:
    - "POST /api/v1/monitoring/register accepts Reddit URL and creates shadow_table entry"
    - "GET /api/v1/monitoring/dashboard returns aggregate stats (active/removed/shadowbanned counts + success rate)"
    - "Background monitoring worker performs dual-check and updates post status"
    - "Scheduler dispatches pending monitoring checks every 15 minutes"
    - "On shadowban detection (2 consecutive checks), system triggers email alert and pattern extraction"
    - "7-day audit task classifies outcome and stops monitoring"
  artifacts:
    - path: "bc-rao-api/app/api/v1/monitoring.py"
      provides: "FastAPI monitoring endpoints (register, list, detail, dashboard)"
      exports: ["router"]
    - path: "bc-rao-api/app/workers/monitoring_worker.py"
      provides: "Background monitoring tasks (check, dispatch, audit, feedback loop)"
      contains: "run_monitoring_check"
    - path: "bc-rao-api/app/workers/task_runner.py"
      provides: "Updated task runner with monitoring background task launcher"
      contains: "run_monitoring_background"
  key_links:
    - from: "bc-rao-api/app/api/v1/monitoring.py"
      to: "bc-rao-api/app/services/monitoring_service.py"
      via: "MonitoringService method calls"
      pattern: "MonitoringService.*register_post|get_monitored_posts|get_dashboard_stats"
    - from: "bc-rao-api/app/workers/monitoring_worker.py"
      to: "bc-rao-api/app/integrations/reddit_client.py"
      via: "RedditDualCheckClient.dual_check_post"
      pattern: "dual_check_post"
    - from: "bc-rao-api/app/workers/monitoring_worker.py"
      to: "bc-rao-api/app/services/email_service.py"
      via: "send_shadowban_alert on detection"
      pattern: "send_shadowban_alert"
    - from: "bc-rao-api/app/workers/monitoring_worker.py"
      to: "bc-rao-api/app/analysis/pattern_extractor.py"
      via: "extract patterns from removed post content"
      pattern: "pattern_extractor|extract.*pattern"
---

<objective>
Build monitoring API endpoints and the background monitoring worker. Endpoints handle post registration, listing monitored posts, dashboard stats, and individual post detail. The worker performs scheduled dual-checks, detects shadowbans via consecutive failure logic, triggers email alerts, extracts forbidden patterns from removed content, and runs 7-day audits.

Purpose: This connects the monitoring service layer (Plan 01) to the HTTP API and the background task system, making monitoring functional end-to-end from registration to feedback loop.

Output: FastAPI router with 5 endpoints, a monitoring worker with 4 background task functions, and updated task_runner integration.
</objective>

<execution_context>
@C:\Users\quena\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\quena\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-monitoring-feedback-loop/05-CONTEXT.md
@.planning/phases/05-monitoring-feedback-loop/05-RESEARCH.md
@.planning/phases/05-monitoring-feedback-loop/05-01-SUMMARY.md

@bc-rao-api/app/api/v1/router.py
@bc-rao-api/app/api/v1/drafts.py
@bc-rao-api/app/workers/task_runner.py
@bc-rao-api/app/workers/generation_worker.py
@bc-rao-api/app/analysis/pattern_extractor.py
@bc-rao-api/app/dependencies.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: FastAPI monitoring endpoints</name>
  <files>
    bc-rao-api/app/api/v1/monitoring.py
    bc-rao-api/app/api/v1/router.py
  </files>
  <action>
**api/v1/monitoring.py** — Create FastAPI router with monitoring endpoints following existing patterns in drafts.py:

1. `POST /monitoring/register` — Register a posted draft for monitoring
   - Body: `RegisterPostRequest` (post_url, campaign_id)
   - Auth: Extract user_id from JWT via `get_current_user` dependency
   - Call `MonitoringService.register_post(user_id, campaign_id, post_url)`
   - On success: schedule first monitoring check via background task (explained in Task 2)
   - On success: schedule 7-day audit via background task with ETA
   - Return 201 with `RegisterPostResponse`
   - Error 400 for invalid URL format, 409 for duplicate post

2. `GET /monitoring/posts` — List monitored posts for a campaign
   - Query params: `campaign_id: UUID` (required), `status: str | None` (optional filter)
   - Auth: user_id from JWT
   - Call `MonitoringService.get_monitored_posts(user_id, campaign_id, status)`
   - Return 200 with `{"posts": list[ShadowEntry], "total": int}`

3. `GET /monitoring/dashboard` — Aggregate monitoring stats
   - Query params: `campaign_id: UUID` (required)
   - Auth: user_id from JWT
   - Call `MonitoringService.get_dashboard_stats(user_id, campaign_id)`
   - Return 200 with `MonitoringDashboardStats`

4. `GET /monitoring/{shadow_id}` — Single monitored post detail
   - Auth: user_id from JWT
   - Call `MonitoringService.get_shadow_entry(shadow_id, user_id)`
   - Return 200 with `ShadowEntry` including check_history from metadata
   - Error 404 if not found or wrong user

5. `GET /monitoring/stream/{task_id}` — SSE endpoint for monitoring check progress
   - Same SSE pattern as collection and generation: poll Redis task state via `get_task_state(task_id)`, yield JSON events every 500ms
   - No auth (task_id is UUID bearer token per project decision)
   - Yield events until state is SUCCESS or FAILURE

**api/v1/router.py** — Add the monitoring router:
   - Import monitoring router from `app.api.v1.monitoring`
   - Add `router.include_router(monitoring.router, prefix="/monitoring", tags=["monitoring"])`

All endpoints follow existing patterns: try/except wrapping, standardized error responses, proper HTTP status codes.
  </action>
  <verify>
    python -c "from app.api.v1.monitoring import router; print(f'Routes: {len(router.routes)}'); print('Monitoring router OK')"
  </verify>
  <done>
    5 monitoring endpoints registered under /api/v1/monitoring prefix. POST /register creates shadow entry, GET /posts lists with filter, GET /dashboard returns stats, GET /{shadow_id} returns detail, GET /stream/{task_id} provides SSE. Router included in main API router.
  </done>
</task>

<task type="auto">
  <name>Task 2: Background monitoring worker with scheduler + feedback loop</name>
  <files>
    bc-rao-api/app/workers/monitoring_worker.py
    bc-rao-api/app/workers/task_runner.py
  </files>
  <action>
**workers/monitoring_worker.py** — Create monitoring background tasks following the asyncio pattern from task_runner.py (NOT Celery, per project decision to use asyncio for Railway):

1. `async def run_monitoring_check(task_id: str, shadow_id: str)`:
   - Import MonitoringService, RedditDualCheckClient, email_service
   - Update Redis task state: STARTED
   - Fetch shadow entry from DB
   - Call `reddit_client.dual_check_post(entry.reddit_post_id)` — returns "active"/"removed"/"shadowbanned"
   - Build check_result dict: `{"timestamp": now_iso, "auth_status": ..., "anon_status": ..., "detected_status": ...}`
   - Implement **consecutive failure logic**: fetch last check result from shadow entry metadata. Only flag "shadowbanned" if BOTH current AND previous check detected shadowban (2 consecutive auth=ok + anon=fail). If only 1 check shows shadowban, keep status as "active" and schedule another check sooner (30 min).
   - Call `monitoring_service.update_post_status(shadow_id, new_status, check_result)`
   - **On shadowban detection (confirmed by 2 consecutive checks)**:
     a. Check `email_service.can_send_shadowban_alert(user_id)` — max 1 per 24h
     b. If can send: call `email_service.send_shadowban_alert(...)` with post details
     c. Record alert in email_alerts table
     d. Trigger pattern extraction: fetch original draft text from generated_drafts (if draft_id exists), call pattern extraction function, inject extracted patterns into syntax_blacklist with `source_post_id = shadow_id`, `failure_type = 'Shadowban'`
   - **On removal detection**: Same pattern extraction + blacklist injection but with `failure_type = 'AdminRemoval'`
   - Update Redis task state: SUCCESS with result details
   - Schedule next check: update `next_check_at` in shadow_table based on check_interval_hours (unless post is removed/shadowbanned — stop monitoring)

2. `async def dispatch_pending_checks()`:
   - Query shadow_table: `WHERE status_vida IN ('Ativo') AND next_check_at <= NOW()`
   - For each pending post, generate a task_id and launch `run_monitoring_check` as asyncio background task
   - Stagger launches: `asyncio.sleep(2)` between each to respect Reddit rate limits
   - Log count of dispatched checks

3. `async def run_post_audit(task_id: str, shadow_id: str)`:
   - Fetch shadow entry
   - If status is already "Shadowbanned" or "Removido": classify as "Rejection" without API call
   - If status "Ativo": call `reddit_client.fetch_post_metrics(reddit_post_id)` to get upvotes + comments
   - Call `monitoring_service.run_post_audit(shadow_id, upvotes, comments)`
   - Update shadow_table status_vida to "Auditado"
   - Stop monitoring (clear next_check_at)
   - Update Redis: SUCCESS with audit outcome

4. `async def extract_and_inject_patterns(shadow_id: str, draft_text: str, subreddit: str, failure_type: str)`:
   - Use the EXISTING pattern_extractor.py `check_post_penalties(text)` function to extract penalty patterns from the removed post's text
   - For each extracted pattern: insert into syntax_blacklist via supabase with:
     - `subreddit`: the post's subreddit
     - `forbidden_pattern`: the matched phrase
     - `failure_type`: 'Shadowban' or 'AdminRemoval' (cast to failure_category enum)
     - `source_post_id`: shadow_id
     - `confidence`: 0.5 for auto-extracted (medium confidence)
     - `is_global`: False (subreddit-specific)
   - Use upsert with `on_conflict=ignore` to avoid duplicate pattern errors (UNIQUE constraint on subreddit+forbidden_pattern)
   - Log count of patterns injected

**workers/task_runner.py** — Add monitoring functions to the existing task runner:

- Add `async def run_monitoring_check_background(task_id: str, shadow_id: str)`:
  Import and call `monitoring_worker.run_monitoring_check(task_id, shadow_id)`

- Add `async def schedule_periodic_monitoring()`:
  Set up a recurring asyncio task that calls `dispatch_pending_checks()` every 15 minutes. Use `asyncio.create_task` with a while-True loop + `asyncio.sleep(900)`.

- Add `async def run_audit_background(task_id: str, shadow_id: str)`:
  Import and call `monitoring_worker.run_post_audit(task_id, shadow_id)`

Pattern extraction reuses the EXISTING `check_post_penalties()` from `app/analysis/pattern_extractor.py` — do NOT create a new spaCy-based extractor. The existing regex-based extractor already covers promotional, self-referential, link patterns, spam indicators, and off-topic categories. Per research open question #3, start with regex extraction (free) before considering LLM-enhanced extraction.
  </action>
  <verify>
    python -c "from app.workers.monitoring_worker import run_monitoring_check, dispatch_pending_checks, run_post_audit, extract_and_inject_patterns; print('Worker imports OK')"
  </verify>
  <done>
    Monitoring worker has 4 async functions: run_monitoring_check (dual-check with consecutive failure logic), dispatch_pending_checks (15-min scheduler), run_post_audit (7-day classification), extract_and_inject_patterns (negative reinforcement to blacklist). Task runner updated with monitoring launchers and periodic scheduler. Pattern extraction reuses existing pattern_extractor.py. Shadowban detection requires 2 consecutive checks before flagging. Email alert rate-limited to 1 per 24h per user.
  </done>
</task>

</tasks>

<verification>
1. All monitoring endpoints accessible under /api/v1/monitoring prefix
2. POST /monitoring/register returns 201 with shadow entry data
3. GET /monitoring/dashboard returns correct stat aggregation
4. Monitoring worker imports cleanly and functions have correct async signatures
5. Consecutive failure logic requires 2 checks before shadowban flag
6. Pattern extraction calls existing check_post_penalties, injects to syntax_blacklist
7. SSE endpoint follows same pattern as collection/generation streams
</verification>

<success_criteria>
- 5 API endpoints registered and returning correct response shapes
- Background worker performs dual-check -> consecutive failure detection -> email alert -> pattern extraction pipeline
- Scheduler queries shadow_table for due checks and dispatches them with rate-limit staggering
- 7-day audit classifies outcomes and stops monitoring
- Pattern extraction uses existing regex extractor and injects to syntax_blacklist with proper failure_type
- All async functions follow existing task_runner.py patterns (Redis state tracking, try/except error handling)
</success_criteria>

<output>
After completion, create `.planning/phases/05-monitoring-feedback-loop/05-02-SUMMARY.md`
</output>
