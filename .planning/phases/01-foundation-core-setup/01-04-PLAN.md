---
phase: 01-foundation-core-setup
plan: 04
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - bc-rao-api/app/workers/celery_app.py
  - bc-rao-api/app/workers/__init__.py
  - bc-rao-api/app/inference/__init__.py
  - bc-rao-api/app/inference/client.py
  - bc-rao-api/app/inference/router.py
  - bc-rao-api/app/inference/cost_tracker.py
  - bc-rao-api/app/services/usage_service.py
  - bc-rao-api/docker-compose.yml
autonomous: true

must_haves:
  truths:
    - "Celery worker starts and connects to Redis broker"
    - "A test task can be dispatched and its result retrieved"
    - "InferenceClient routes tasks to correct models per config"
    - "Cost tracker checks plan cap before allowing inference calls"
    - "Redis and Celery are running via docker-compose"
  artifacts:
    - path: "bc-rao-api/app/workers/celery_app.py"
      provides: "Celery app with Redis broker and queue configuration"
      exports: ["celery_app"]
    - path: "bc-rao-api/app/inference/client.py"
      provides: "InferenceClient abstraction for OpenRouter"
      exports: ["InferenceClient"]
    - path: "bc-rao-api/app/inference/router.py"
      provides: "Model routing configuration per task type"
      exports: ["MODEL_ROUTING"]
    - path: "bc-rao-api/app/inference/cost_tracker.py"
      provides: "Cost checking before inference calls"
      exports: ["CostTracker"]
    - path: "bc-rao-api/app/services/usage_service.py"
      provides: "Usage tracking and limit enforcement"
      exports: ["UsageService"]
    - path: "bc-rao-api/docker-compose.yml"
      provides: "Redis service for Celery broker"
      contains: "redis"
  key_links:
    - from: "bc-rao-api/app/inference/client.py"
      to: "bc-rao-api/app/inference/router.py"
      via: "MODEL_ROUTING lookup by task name"
      pattern: "MODEL_ROUTING"
    - from: "bc-rao-api/app/inference/client.py"
      to: "bc-rao-api/app/inference/cost_tracker.py"
      via: "cost check before inference call"
      pattern: "cost_tracker\\.check|can_afford"
    - from: "bc-rao-api/app/workers/celery_app.py"
      to: "bc-rao-api/app/config.py"
      via: "Redis URL from settings"
      pattern: "REDIS_URL|CELERY_BROKER"
---

<objective>
Worker infrastructure: Celery + Redis task queue with separate queues, InferenceClient abstraction for OpenRouter with model routing, and cost tracking middleware.

Purpose: Background tasks (collection, analysis, generation, monitoring) need the worker infrastructure. The inference layer is needed by Modules 1-3. Cost tracking prevents budget overruns.
Output: Running Celery worker, InferenceClient ready for use, cost tracking enforcing plan caps.
</objective>

<execution_context>
@C:\Users\quena\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\quena\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation-core-setup/01-RESEARCH.md
@.planning/phases/01-foundation-core-setup/01-01-SUMMARY.md
@bc-rao-system-spec_1.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Celery + Redis worker infrastructure with docker-compose</name>
  <files>
    bc-rao-api/app/workers/__init__.py
    bc-rao-api/app/workers/celery_app.py
    bc-rao-api/docker-compose.yml
  </files>
  <action>
**docker-compose.yml:**
- Redis 7 service exposed on port 6379
- Optional: include the FastAPI app service and Celery worker service for convenience
- Redis uses `redis:7-alpine` image
- Network configuration for services to communicate

```yaml
services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  # Worker service (for running Celery)
  worker:
    build: .
    command: celery -A app.workers.celery_app worker --loglevel=info -Q scraping,analysis,generation,monitoring
    env_file: .env
    depends_on:
      - redis

volumes:
  redis_data:
```

**app/workers/celery_app.py:**
- Create Celery app named "bc-rao"
- Broker: settings.CELERY_BROKER_URL (Redis URL)
- Result backend: settings.CELERY_RESULT_BACKEND (Redis URL)
- Configuration:
  - `task_serializer = "json"` (NOT pickle — security)
  - `result_serializer = "json"`
  - `accept_content = ["json"]`
  - `timezone = "UTC"`
  - `enable_utc = True`
  - `task_track_started = True`
  - `task_acks_late = True` (tasks re-queued on worker crash)
- Define 4 task queues from system spec: `scraping`, `analysis`, `generation`, `monitoring`
- Configure default queue routing:
  - `task_routes` mapping task name patterns to queues
- Create a simple `ping` test task that returns "pong" (for verification)
- Use `autoretry_for` pattern on the ping task as a template

**app/workers/__init__.py:**
- Re-export celery_app for convenience
  </action>
  <verify>
1. `docker-compose up -d redis` starts Redis
2. `celery -A app.workers.celery_app worker --loglevel=info` starts worker, connects to Redis
3. From Python shell: `from app.workers.celery_app import ping; result = ping.delay(); print(result.get(timeout=5))` returns "pong"
  </verify>
  <done>Redis running via docker-compose, Celery worker starts with 4 queues (scraping, analysis, generation, monitoring), test task dispatches and returns result.</done>
</task>

<task type="auto">
  <name>Task 2: InferenceClient with model routing and cost tracking</name>
  <files>
    bc-rao-api/app/inference/__init__.py
    bc-rao-api/app/inference/client.py
    bc-rao-api/app/inference/router.py
    bc-rao-api/app/inference/cost_tracker.py
    bc-rao-api/app/services/usage_service.py
  </files>
  <action>
**app/inference/router.py:**
- Define `MODEL_ROUTING` dict exactly as in system spec Section 6.1:
  - `classify_archetype`: claude-3-haiku, max_tokens=200, temp=0.1, fallback=gemini-flash
  - `generate_draft`: claude-sonnet-4, max_tokens=2000, temp=0.7, fallback=gpt-4o-mini
  - `score_post`: claude-3-haiku, max_tokens=500, temp=0.0, fallback=gemini-flash
  - `extract_patterns`: claude-3-haiku, max_tokens=1000, temp=0.2, fallback=gemini-flash
- Define `COST_CAPS` dict from spec: trial=$5, starter=$15, growth=$40

**app/inference/cost_tracker.py:**
- `CostTracker` class
- `check_budget(user_id: str, plan: str) -> tuple[bool, float]`:
  - Query usage_tracking table: SUM(cost_usd) for user in current billing period
  - Compare against COST_CAPS[plan]
  - Return (can_proceed, remaining_budget)
- `record_usage(user_id: str, action_type: str, campaign_id: str | None, token_count: int, cost_usd: float)`:
  - Insert row into usage_tracking table
- `get_usage_summary(user_id: str) -> dict`:
  - Return total cost, action breakdown, remaining budget

**app/inference/client.py:**
- `InferenceClient` class
- Constructor takes `task: str` (key from MODEL_ROUTING)
- Resolves model config from MODEL_ROUTING
- `async call(prompt: str, user_id: str, plan: str, campaign_id: str | None = None) -> dict`:
  1. Check budget via CostTracker — if over cap, raise AppError(PLAN_LIMIT_REACHED)
  2. Call OpenRouter API via httpx:
     - URL: `https://openrouter.ai/api/v1/chat/completions`
     - Headers: `Authorization: Bearer {OPENROUTER_API_KEY}`, `HTTP-Referer: https://bcrao.app`
     - Body: model, messages=[{"role": "user", "content": prompt}], max_tokens, temperature
  3. On failure: try fallback model (same request, different model)
  4. Parse response, extract content and usage (prompt_tokens, completion_tokens)
  5. Calculate cost (rough estimate: prompt_tokens * input_price + completion_tokens * output_price)
  6. Record usage via CostTracker
  7. Return {"content": str, "model_used": str, "token_count": int, "cost_usd": float}
- On OpenRouter error: raise AppError(INFERENCE_FAILED)

**app/services/usage_service.py:**
- `UsageService` class
- Wraps CostTracker for use as a FastAPI dependency
- `get_usage_for_user(user_id: str) -> dict`: returns usage summary
- `check_action_limit(user_id: str, action_type: str, plan: str) -> bool`:
  - For future use: check if specific action type limit exceeded
  - For now: delegates to CostTracker.check_budget

**app/inference/__init__.py:**
- Re-export InferenceClient, MODEL_ROUTING, COST_CAPS
  </action>
  <verify>
1. `python -c "from app.inference.client import InferenceClient; print('OK')"` works
2. `python -c "from app.inference.router import MODEL_ROUTING; print(list(MODEL_ROUTING.keys()))"` shows 4 task types
3. `python -c "from app.inference.cost_tracker import CostTracker; print('OK')"` works
4. `python -c "from app.services.usage_service import UsageService; print('OK')"` works
  </verify>
  <done>InferenceClient calls OpenRouter with model routing per task, falls back on failure, CostTracker checks plan caps before every call and records usage, UsageService provides FastAPI-friendly interface.</done>
</task>

</tasks>

<verification>
1. `docker-compose up -d redis` — Redis running on 6379
2. Celery worker starts and shows 4 queues registered
3. Ping task dispatches and returns "pong"
4. InferenceClient resolves correct model for each task type
5. CostTracker can query usage_tracking table (requires Supabase connection)
6. COST_CAPS match system spec: trial=$5, starter=$15, growth=$40
</verification>

<success_criteria>
- INFR-03: Celery workers and Redis broker are running and accepting tasks with separate queues
- INFR-04: InferenceClient abstracts OpenRouter with per-task model routing and fallback chains
- INFR-05: Cost tracking middleware checks plan cap before every LLM call
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-core-setup/01-04-SUMMARY.md`
</output>
